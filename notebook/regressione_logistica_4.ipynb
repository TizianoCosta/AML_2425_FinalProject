{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook per regressione lineare\n",
        "\n",
        "Di seguito:\n",
        "- verrà implementato l'algoritmo di regressione logistica"
      ],
      "metadata": {
        "id": "CIs0Jhd77joY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMd2ihQ1ZpS6"
      },
      "source": [
        "## Scaricamento dei dati"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vBLt0MJ7fSI"
      },
      "outputs": [],
      "source": [
        "# File per scaricare i dati per fare analisi di machine learning\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "\n",
        "# List of URLs to your zipped files on AWS\n",
        "urls = [\n",
        "    \"https://phm-datasets.s3.amazonaws.com/Data_Challenge_PHM2024_training_data.zip\"]\n",
        "\n",
        "# Directory to save the extracted files\n",
        "output_dir = \"dataset\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for url in urls:\n",
        "    try:\n",
        "        print(f\"Downloading {url}...\")\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "\n",
        "        # Read the zip file from the response content\n",
        "        with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
        "            # Extract all contents to the specified output directory\n",
        "            zip_ref.extractall(output_dir)\n",
        "            print(f\"Extracted files from {url} to {output_dir}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading {url}: {e}\")\n",
        "    except zipfile.BadZipFile:\n",
        "        print(f\"Error: The downloaded file from {url} is not a valid zip file.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "print(\"Download and extraction complete.\")\n",
        "\n",
        "# Now you can access your CSV files in the 'downloaded_data' directory\n",
        "# For example, to list the files in the directory:\n",
        "import glob\n",
        "csv_files = glob.glob(os.path.join(output_dir, \"*.csv\"))\n",
        "print(\"CSV files found:\", csv_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er3mr_yCZyR7"
      },
      "source": [
        "## Estrazione dei dati"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRFm7gIdR48e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_data(x_path, y_path):\n",
        "  \"\"\"\n",
        "  Loads X.csv and the second column of y.csv into a single pandas DataFrame.\n",
        "\n",
        "  Args:\n",
        "    x_path (str): The path to the X.csv file.\n",
        "    y_path (str): The path to the y.csv file.\n",
        "\n",
        "  Returns:\n",
        "    pandas.DataFrame: A DataFrame containing the data from X.csv\n",
        "                      and the second column of y.csv.\n",
        "  \"\"\"\n",
        "  x = pd.read_csv(x_path)\n",
        "  y = pd.read_csv(y_path)\n",
        "\n",
        "  # Assuming y has at least 2 columns and the second column is at index 1\n",
        "  if y.shape[1] > 1:\n",
        "    combined_data = x.copy()\n",
        "    combined_data['y_target'] = y.iloc[:, 1]\n",
        "    return combined_data\n",
        "  else:\n",
        "    print(\"Error: y.csv does not have a second column.\")\n",
        "    return x\n",
        "\n",
        "# Example usage:\n",
        "# Assuming your files are in the 'dataset' directory as per the preceding code\n",
        "x_path = 'dataset/X_train.csv'\n",
        "y_path = 'dataset/y_train.csv'\n",
        "\n",
        "data = load_data(x_path, y_path)\n",
        "\n",
        "# You can now work with the 'data' DataFrame\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_-Gtljcq3v4"
      },
      "source": [
        "## Creazione training-set testing-set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqia2aOJX_MR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_train, data_test = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Dimensione data_train:\", data_train.shape)\n",
        "print(\"Dimensione data_test:\", data_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test"
      ],
      "metadata": {
        "id": "o-wi-88jO4ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PIPELINE"
      ],
      "metadata": {
        "id": "lyQkp8iD3x6d"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lpARpCCgreg"
      },
      "source": [
        "## Cambio nome delle feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPNHbqucguH5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "class FeatureRenamer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, column_names):\n",
        "        if not isinstance(column_names, list):\n",
        "            raise TypeError(\"column_names must be a list.\")\n",
        "        self.column_names = column_names\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        is_tuple = isinstance(X, tuple)\n",
        "        if is_tuple:\n",
        "            if not X:\n",
        "                raise ValueError(\"Input tuple is empty.\")\n",
        "            df = X[0]\n",
        "            rest = X[1:]\n",
        "        else:\n",
        "            df = X\n",
        "            rest = ()\n",
        "\n",
        "        if not isinstance(df, pd.DataFrame):\n",
        "            raise TypeError(\"Input (or first element if tuple) must be a DataFrame.\")\n",
        "\n",
        "        # Drop 'id' column if present\n",
        "        if 'id' in df.columns:\n",
        "            df = df.drop(columns='id')\n",
        "\n",
        "        # Check column count matches\n",
        "        if len(self.column_names) != df.shape[1]:\n",
        "            if len(self.column_names) + 1 == df.shape[1] and 'faulty' in df.columns:\n",
        "                print(\"Detected 'faulty' column, adjusting rename.\")\n",
        "                new_cols = self.column_names + ['faulty']\n",
        "                if len(new_cols) != df.shape[1]:\n",
        "                    raise ValueError(\"Mismatch in column count with 'faulty' included.\")\n",
        "                df_renamed = df.copy()\n",
        "                df_renamed.columns = new_cols\n",
        "            else:\n",
        "                raise ValueError(\"Column count mismatch after dropping 'id'.\")\n",
        "        else:\n",
        "            df_renamed = df.copy()\n",
        "            df_renamed.columns = self.column_names\n",
        "\n",
        "        return (df_renamed,) + rest if rest else df_renamed\n",
        "\n",
        "# ---------------------- Setup ----------------------\n",
        "new_names_for_train_data = [\n",
        "    'torque_meas', 'outside_air_temp', 'mean_gas_temp',\n",
        "    'power_avail', 'indicated_air_speed', 'net_power',\n",
        "    'compressor_speed', 'health_state'\n",
        "]\n",
        "\n",
        "# ---------------------- Pipeline ----------------------\n",
        "pipeline_with_renaming = Pipeline([\n",
        "    ('rename_features', FeatureRenamer(column_names=new_names_for_train_data))\n",
        "])\n",
        "\n",
        "# Apply pipeline\n",
        "data_train_renamed = pipeline_with_renaming.fit_transform(data_train)\n",
        "\n",
        "# ---------------------- Output ----------------------\n",
        "print(\"Renamed DataFrame (without 'id'):\")\n",
        "print(data_train_renamed.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVwb2LUAdtDE"
      },
      "source": [
        "## Standardizzazione"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def standardize_columns(df, columns_to_standardize):\n",
        "  \"\"\"\n",
        "  Standardizes specified columns of a pandas DataFrame to have values between 0 and 1\n",
        "  using MinMaxScaler.\n",
        "\n",
        "  Args:\n",
        "    df: The pandas DataFrame to standardize.\n",
        "    columns_to_standardize: A list of column names to standardize.\n",
        "\n",
        "  Returns:\n",
        "    The DataFrame with the specified columns standardized.\n",
        "  \"\"\"\n",
        "  scaler = MinMaxScaler()\n",
        "  df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n",
        "  return df\n",
        "\n",
        "# Example usage:\n",
        "# Assuming you want to standardize all numerical columns except the index and the target variable\n",
        "# Identify numerical columns (excluding 'idx' and 'health_state' in this case)\n",
        "numerical_cols = data_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "columns_to_standardize = [col for col in numerical_cols if col not in ['idx', 'health_state']]\n",
        "\n",
        "print(f\"\\nColumns to standardize: {columns_to_standardize}\")\n",
        "\n",
        "data_train_standardized = standardize_columns(data_train.copy(), columns_to_standardize)\n",
        "\n",
        "print(\"\\nDataFrame after standardization:\")\n",
        "print(data_train_standardized.head())\n",
        "print(\"\\nDescriptive statistics after standardization:\")\n",
        "print(data_train_standardized.describe())"
      ],
      "metadata": {
        "id": "ZwxwR03k--v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crea una classe che sia compatibile con l'oggetto pipeline di scikit che contenga la funzione transform_with_custom_root standardize_columns\n",
        "\n",
        "class ColumnStandardizer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns_to_standardize=None):\n",
        "        self.columns_to_standardize = columns_to_standardize\n",
        "        self.scaler = MinMaxScaler()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            raise TypeError(\"Input must be a DataFrame.\")\n",
        "\n",
        "        if self.columns_to_standardize is None:\n",
        "            # Identify numerical columns excluding the target if present\n",
        "            numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "            # Assuming 'y_target' or similar is the target and should not be scaled\n",
        "            # This part might need adjustment based on how your pipeline handles the target column\n",
        "            self._cols_to_fit = [col for col in numerical_cols if col not in ['y_target', 'health_state']] # Exclude common target/index names\n",
        "        else:\n",
        "             if not isinstance(self.columns_to_standardize, (list, tuple)):\n",
        "                 raise TypeError(\"columns_to_standardize must be a list or tuple of column names.\")\n",
        "             # Ensure all specified columns exist in the input DataFrame\n",
        "             missing_cols = [col for col in self.columns_to_standardize if col not in X.columns]\n",
        "             if missing_cols:\n",
        "                 raise ValueError(f\"Columns not found in input DataFrame: {missing_cols}\")\n",
        "             self._cols_to_fit = self.columns_to_standardize\n",
        "\n",
        "        if not self._cols_to_fit:\n",
        "            print(\"Warning: No columns selected for standardization.\")\n",
        "            return self\n",
        "\n",
        "        # Fit the scaler only on the selected numerical columns\n",
        "        self.scaler.fit(X[self._cols_to_fit])\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            raise TypeError(\"Input must be a DataFrame.\")\n",
        "        if not hasattr(self, '_cols_to_fit'):\n",
        "             raise RuntimeError(\"The transformer has not been fitted yet.\")\n",
        "\n",
        "        df_standardized = X.copy()\n",
        "\n",
        "        if not self._cols_to_fit:\n",
        "             return df_standardized # Return original if no columns to standardize\n",
        "\n",
        "        # Ensure that the columns to transform are actually present in the input DataFrame\n",
        "        present_cols_to_transform = [col for col in self._cols_to_fit if col in df_standardized.columns]\n",
        "\n",
        "        if not present_cols_to_transform:\n",
        "             print(\"Warning: None of the specified columns for standardization are present in the input DataFrame.\")\n",
        "             return df_standardized\n",
        "\n",
        "        # Transform only the present columns\n",
        "        df_standardized[present_cols_to_transform] = self.scaler.transform(df_standardized[present_cols_to_transform])\n",
        "\n",
        "        return df_standardized\n"
      ],
      "metadata": {
        "id": "REDgtiwT_AGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rimozione colonne"
      ],
      "metadata": {
        "id": "7_oDSKMkD1yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crea una trasformazione che droppi certe colonne di un dataset\n",
        "\n",
        "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns_to_drop):\n",
        "        if not isinstance(columns_to_drop, (list, tuple)):\n",
        "            raise TypeError(\"columns_to_drop must be a list or tuple of column names.\")\n",
        "        self.columns_to_drop = columns_to_drop\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Nothing to fit, but check if columns exist in the input\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            raise TypeError(\"Input must be a DataFrame.\")\n",
        "        missing_cols = [col for col in self.columns_to_drop if col not in X.columns]\n",
        "        if missing_cols:\n",
        "            print(f\"Warning: Columns to drop not found in input DataFrame: {missing_cols}. These will be ignored during transform.\")\n",
        "            # Store the columns that are actually present to avoid errors during transform\n",
        "            self._cols_to_drop_present = [col for col in self.columns_to_drop if col in X.columns]\n",
        "        else:\n",
        "            self._cols_to_drop_present = self.columns_to_drop\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            raise TypeError(\"Input must be a DataFrame.\")\n",
        "        if not hasattr(self, '_cols_to_drop_present'):\n",
        "             raise RuntimeError(\"The transformer has not been fitted yet.\")\n",
        "\n",
        "        df_dropped = X.copy()\n",
        "        # Drop only the columns that were found during fit\n",
        "        df_dropped = df_dropped.drop(columns=self._cols_to_drop_present, errors='ignore')\n",
        "        return df_dropped"
      ],
      "metadata": {
        "id": "gtiVf-IWD0-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Radice custom"
      ],
      "metadata": {
        "id": "ZSURPvKHuL8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trasformazione in work_with_data"
      ],
      "metadata": {
        "id": "0tSW1aPUtxV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_with_custom_root(df, column_name, root_degree):\n",
        "  \"\"\"\n",
        "  Applies a custom root transformation (1/root_degree power) to a column.\n",
        "  Handles positive, negative, and zero values appropriately based on the root degree.\n",
        "\n",
        "  Args:\n",
        "    df (pd.DataFrame): The input DataFrame.\n",
        "    column_name (str): The name of the column to transform.\n",
        "    root_degree (float): The degree of the root (e.g., 2 for square root, 3 for cube root).\n",
        "\n",
        "  Returns:\n",
        "    pd.DataFrame: The DataFrame with the transformed column.\n",
        "  \"\"\"\n",
        "  new_column_name = f'{column_name}_root_{root_degree:.2f}_transformed'\n",
        "\n",
        "  if root_degree == 0:\n",
        "      raise ValueError(\"Root degree cannot be zero.\")\n",
        "  elif root_degree % 2 == 0:  # Even root\n",
        "      # For even roots, we can only take the root of non-negative numbers\n",
        "      if (df[column_name] < 0).any():\n",
        "          print(f\"Warning: Column '{column_name}' contains negative values. Cannot apply even root directly.\")\n",
        "          # You might choose to handle this by taking the root of the absolute value,\n",
        "          # or setting negative values to NaN, depending on your data context.\n",
        "          # Here, we'll take the root of the absolute value for demonstration.\n",
        "          df[new_column_name] = np.power(np.abs(df[column_name]), 1/root_degree)\n",
        "      else:\n",
        "          df[new_column_name] = np.power(df[column_name], 1/root_degree)\n",
        "  else:  # Odd root\n",
        "      # Odd roots can handle positive, negative, and zero values\n",
        "      df[new_column_name] = np.sign(df[column_name]) * np.power(np.abs(df[column_name]), 1/root_degree)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "_UAxC3Cosvjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trasformazione pipeline"
      ],
      "metadata": {
        "id": "xnmnqu9lwe61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crea una classe che sia compatibile con l'oggetto pipeline di scikit che contenga la funzione transform_with_custom_root\n",
        "\n",
        "class CustomRootTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, root_transformations=None):\n",
        "        self.root_transformations = root_transformations\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
        "\n",
        "        df_transformed = X.copy()\n",
        "\n",
        "        if self.root_transformations:\n",
        "            for col, deg in self.root_transformations.items():\n",
        "                if col not in df_transformed.columns:\n",
        "                    # print(f\"Warning: Column '{col}' not found in DataFrame. Skipping root transformation.\") # Silence this\n",
        "                    continue\n",
        "\n",
        "                # print(f\"Applying custom root {deg} transformation to column '{col}'...\") # Silence this\n",
        "                # Use the existing transform_with_custom_root function but handle its potential print\n",
        "                # A more robust way would be to modify transform_with_custom_root itself\n",
        "                # For this case, we assume transform_with_custom_root doesn't print excessively\n",
        "                # or we accept its prints for now. To strictly silence, you'd need to redirect stdout.\n",
        "                df_transformed = transform_with_custom_root(df_transformed, col, deg)\n",
        "\n",
        "        return df_transformed\n",
        "\n"
      ],
      "metadata": {
        "id": "C0HoEko5tT-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantile transformer"
      ],
      "metadata": {
        "id": "G_3M8alDn9FZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trasformazione in work_with_data"
      ],
      "metadata": {
        "id": "o6s1Pf27oCJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "def transform_with_quantile_transformer(df, column_name, output_distribution='normal'):\n",
        "  \"\"\"\n",
        "  Applies a quantile transformation to a specified column to make its distribution\n",
        "  more Gaussian-like (normal) or uniform.\n",
        "\n",
        "  Args:\n",
        "    df (pd.DataFrame): The input DataFrame.\n",
        "    column_name (str): The name of the column to transform.\n",
        "    output_distribution (str): The desired output distribution. 'normal'\n",
        "                               for Gaussian-like, 'uniform' for uniform.\n",
        "                               Default is 'normal'.\n",
        "\n",
        "  Returns:\n",
        "    pd.DataFrame: The DataFrame with the transformed column.\n",
        "  \"\"\"\n",
        "  if column_name not in df.columns:\n",
        "    raise ValueError(f\"La colonna '{column_name}' non è presente nel DataFrame.\")\n",
        "\n",
        "  # Reshape the data as the transformer expects a 2D array\n",
        "  data_to_transform = df[column_name].values.reshape(-1, 1)\n",
        "\n",
        "  # Initialize and fit the QuantileTransformer\n",
        "  quantile_transformer = QuantileTransformer(output_distribution=output_distribution)\n",
        "  transformed_data = quantile_transformer.fit_transform(data_to_transform)\n",
        "\n",
        "  # Add the transformed data back to the DataFrame with a new column name\n",
        "  df[f'{column_name}_quantile_transformed'] = transformed_data\n",
        "\n",
        "  return df\n",
        "\n",
        "# Apply the quantile transformation to the 'power_avail' column\n",
        "#data_train = transform_with_quantile_transformer(data_train.copy(), 'power_avail', output_distribution='normal')\n",
        "\n"
      ],
      "metadata": {
        "id": "TEj45O3YbkGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trasformazione pipeline"
      ],
      "metadata": {
        "id": "wZcqAj9roCJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crea una classe che sia compatibile con l'oggetto pipeline di scikit che contenga la funzione transform_with_quantile_transformer\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "class QuantileTransformerWrapper(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns_to_transform, output_distribution='normal'):\n",
        "        \"\"\"\n",
        "        Initializes the QuantileTransformerWrapper.\n",
        "\n",
        "        Args:\n",
        "          columns_to_transform (list): A list of column names to transform.\n",
        "          output_distribution (str): The desired output distribution. 'normal'\n",
        "                                     for Gaussian-like, 'uniform' for uniform.\n",
        "                                     Default is 'normal'.\n",
        "        \"\"\"\n",
        "        if not isinstance(columns_to_transform, (list, tuple)):\n",
        "            raise TypeError(\"columns_to_transform must be a list or tuple of column names.\")\n",
        "        self.columns_to_transform = columns_to_transform\n",
        "        self.output_distribution = output_distribution\n",
        "        self.transformers = {} # Dictionary to store transformers for each column\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fits a QuantileTransformer for each specified column.\n",
        "\n",
        "        Args:\n",
        "          X (pd.DataFrame): The input DataFrame.\n",
        "          y: Ignored.\n",
        "\n",
        "        Returns:\n",
        "          self: The fitted transformer.\n",
        "        \"\"\"\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
        "\n",
        "        for col in self.columns_to_transform:\n",
        "            if col not in X.columns:\n",
        "                print(f\"Warning: Column '{col}' not found in DataFrame during fit. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Initialize and fit the QuantileTransformer for the current column\n",
        "            transformer = QuantileTransformer(output_distribution=self.output_distribution)\n",
        "            transformer.fit(X[col].values.reshape(-1, 1))\n",
        "            self.transformers[col] = transformer\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Applies the fitted QuantileTransformer to the specified columns.\n",
        "\n",
        "        Args:\n",
        "          X (pd.DataFrame): The input DataFrame.\n",
        "\n",
        "        Returns:\n",
        "          pd.DataFrame: The DataFrame with the transformed columns.\n",
        "        \"\"\"\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
        "        if not hasattr(self, 'transformers'):\n",
        "            raise RuntimeError(\"The transformer has not been fitted yet.\")\n",
        "\n",
        "        df_transformed = X.copy()\n",
        "\n",
        "        for col, transformer in self.transformers.items():\n",
        "            if col in df_transformed.columns:\n",
        "                # Apply the transformation\n",
        "                transformed_data = transformer.transform(df_transformed[col].values.reshape(-1, 1))\n",
        "                # Add the transformed data back to the DataFrame with a new column name\n",
        "                df_transformed[f'{col}_quantile_transformed'] = transformed_data\n",
        "            else:\n",
        "                 print(f\"Warning: Column '{col}' not found in DataFrame during transform. Skipping.\")\n",
        "\n",
        "\n",
        "        return df_transformed\n"
      ],
      "metadata": {
        "id": "eRpoeEteblPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binary transformation"
      ],
      "metadata": {
        "id": "CKNFnVhcuRiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trasformazione work_with_data"
      ],
      "metadata": {
        "id": "teNr1y8MwUHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_binned_qualitative_variable(df, column_name, num_bins, strategy='quantile'):\n",
        "  \"\"\"\n",
        "  Creates a qualitative (categorical) variable by binning a numerical column.\n",
        "\n",
        "  Args:\n",
        "    df (pd.DataFrame): The input DataFrame.\n",
        "    column_name (str): The name of the numerical column to bin.\n",
        "    num_bins (int): The desired number of bins.\n",
        "    strategy (str): The strategy to use for binning. 'quantile' uses quantiles\n",
        "                    to ensure bins have approximately equal numbers of observations.\n",
        "                    'uniform' creates bins with equal widths. Default is 'quantile'.\n",
        "\n",
        "  Returns:\n",
        "    pd.DataFrame: The DataFrame with a new qualitative column.\n",
        "                  The new column name will be f'{column_name}_binned_{num_bins}_{strategy}'.\n",
        "  \"\"\"\n",
        "  if column_name not in df.columns:\n",
        "    raise ValueError(f\"La colonna '{column_name}' non è presente nel DataFrame.\")\n",
        "  if num_bins <= 1:\n",
        "      raise ValueError(\"Il numero di bins deve essere maggiore di 1.\")\n",
        "\n",
        "  new_column_name = f'{column_name}_binned_{num_bins}_{strategy}'\n",
        "\n",
        "  if strategy == 'quantile':\n",
        "    # Use qcut to create bins based on quantiles (approximately equal number of observations)\n",
        "    # `duplicates='drop'` handles cases where quantile boundaries are not unique,\n",
        "    # which can happen with skewed or discrete data.\n",
        "    df[new_column_name] = pd.qcut(df[column_name], q=num_bins, labels=False, duplicates='drop')\n",
        "  elif strategy == 'uniform':\n",
        "    # Use cut to create bins of equal width\n",
        "    df[new_column_name] = pd.cut(df[column_name], bins=num_bins, labels=False, include_lowest=True)\n",
        "  else:\n",
        "    raise ValueError(f\"Strategia di binning non valida: '{strategy}'. Scegliere tra 'quantile' o 'uniform'.\")\n",
        "\n",
        "  # Convert the binned column to object/category type if needed, or keep as int for simplicity\n",
        "  # Here we keep it as int representing the bin number\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "eSLqiujBulI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trasformazione pipeline"
      ],
      "metadata": {
        "id": "ajBPOBOJutTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crea una classe che sia compatibile con l'oggetto pipeline di scikit che contenga la funzione  create_binned_qualitative_variable\n",
        "\n",
        "class BinnedQualitativeTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, binning_config=None):\n",
        "        self.binning_config = binning_config\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
        "\n",
        "        df_transformed = X.copy()\n",
        "\n",
        "        if self.binning_config:\n",
        "            for col, (num_bins, strategy) in self.binning_config.items():\n",
        "                if col not in df_transformed.columns:\n",
        "                    # print(f\"Warning: Column '{col}' not found in DataFrame. Skipping binning transformation.\") # Silence this\n",
        "                    continue\n",
        "\n",
        "                # print(f\"Applying binning transformation to column '{col}' with {num_bins} bins and strategy '{strategy}'...\") # Silence this\n",
        "                df_transformed = create_binned_qualitative_variable(df_transformed, col, num_bins, strategy)\n",
        "\n",
        "        return df_transformed\n"
      ],
      "metadata": {
        "id": "cXcKS456u7AM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA Compressor speed net power"
      ],
      "metadata": {
        "id": "5t1DNnKZvv0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trasformazione con work_with_data"
      ],
      "metadata": {
        "id": "Tkns2nkBx9wC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#features_for_pca = data_train[['compressor_speed', 'net_power']]\n",
        "\n",
        "\n",
        "#pca = PCA(n_components=1)\n",
        "\n",
        "#data_train['compressor_speed_net_power_pca'] = pca.fit_transform(features_for_pca)"
      ],
      "metadata": {
        "id": "vvEG6L_0xDds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trasformazione pipeline"
      ],
      "metadata": {
        "id": "_upK78g00f3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crea una classe che sia compatibile con l'oggetto pipeline di scikit che  come nella cella sopra a partire dalle feature 'compressor_speed' e net_power' tramite il metodo PCA crei una nuova variabile 'compressor_speed_net_power_pca', droppa poi le feature 'net_power' e 'compressor_speed'\n",
        "\n",
        "class PCATransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns_to_pca, n_components=1, new_column_prefix=\"pca\"):\n",
        "        if not isinstance(columns_to_pca, list) or len(columns_to_pca) < 1:\n",
        "            raise ValueError(\"columns_to_pca must be a list of at least one column name.\")\n",
        "        if n_components < 1 or n_components > len(columns_to_pca):\n",
        "            raise ValueError(\"n_components must be between 1 and the number of columns to PCA.\")\n",
        "\n",
        "        self.columns_to_pca = columns_to_pca\n",
        "        self.n_components = n_components\n",
        "        self.new_column_prefix = new_column_prefix\n",
        "        self.pca_model = None\n",
        "        self.new_column_names = []\n",
        "\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
        "\n",
        "        missing_cols = [col for col in self.columns_to_pca if col not in X.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"The following columns for PCA were not found in the DataFrame: {missing_cols}\")\n",
        "\n",
        "        # print(f\"Fitting PCA on columns: {self.columns_to_pca}\") # Silence this\n",
        "        self.pca_model = PCA(n_components=self.n_components)\n",
        "        self.pca_model.fit(X[self.columns_to_pca])\n",
        "\n",
        "        if self.n_components == 1:\n",
        "            self.new_column_names = [f'{self.new_column_prefix}_{\"_\".join(self.columns_to_pca).replace(\".\", \"_\")}']\n",
        "        else:\n",
        "             self.new_column_names = [f'{self.new_column_prefix}_{i+1}' for i in range(self.n_components)]\n",
        "\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
        "\n",
        "        if self.pca_model is None:\n",
        "            raise RuntimeError(\"PCA model has not been fitted yet. Call fit() first.\")\n",
        "\n",
        "        df_transformed = X.copy()\n",
        "\n",
        "        cols_to_drop_exist = [col for col in self.columns_to_pca if col in df_transformed.columns]\n",
        "\n",
        "        # print(f\"Transforming columns {self.columns_to_pca} using PCA...\") # Silence this\n",
        "        pca_result = self.pca_model.transform(df_transformed[self.columns_to_pca])\n",
        "\n",
        "        if self.n_components == 1:\n",
        "             df_transformed[self.new_column_names[0]] = pca_result[:, 0]\n",
        "        else:\n",
        "            for i, col_name in enumerate(self.new_column_names):\n",
        "                df_transformed[col_name] = pca_result[:, i]\n",
        "\n",
        "        # print(f\"Dropping original PCA columns: {cols_to_drop_exist}\") # Silence this\n",
        "        df_transformed = df_transformed.drop(columns=cols_to_drop_exist)\n",
        "\n",
        "        return df_transformed"
      ],
      "metadata": {
        "id": "WLF6ZxrzvNPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature engineering"
      ],
      "metadata": {
        "id": "Zo2U0HdR1Eh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trasformazione work_with_data"
      ],
      "metadata": {
        "id": "p8qV6N2n1One"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data_train['torque_times_temp'] = data_train['torque_meas'] * data_train['outside_air_temp']#"
      ],
      "metadata": {
        "id": "-tGxNusg0P7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trasformazione pipeline"
      ],
      "metadata": {
        "id": "LE6q0V-01NqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## torque_meas e outside_air_speed"
      ],
      "metadata": {
        "id": "ESNIZeI46BTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crea una classe che sia compatibile con l'oggetto pipeline di scikit che,  come nella cella sopra ,faccia il prodotto dalle feature 'torque_meas' e outside_air_temp' , e crei  una nuova feature 'torque_times_temp', droppa poi le feature ''torque_meas' e 'outside_air_temp\n",
        "\n",
        "\n",
        "class TorqueTempFeature(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        required_cols = ['torque_meas', 'outside_air_temp']\n",
        "        missing_cols = [col for col in required_cols if col not in X.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"The following required columns for TorqueTempFeature were not found in the DataFrame: {missing_cols}\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
        "\n",
        "        df_transformed = X.copy()\n",
        "\n",
        "        required_cols = ['torque_meas', 'outside_air_temp']\n",
        "        missing_cols = [col for col in required_cols if col not in df_transformed.columns]\n",
        "\n",
        "        if missing_cols:\n",
        "             # print(f\"Warning: Skipping 'torque_times_temp' creation as columns are missing: {missing_cols}\") # Silence this\n",
        "             return df_transformed\n",
        "        else:\n",
        "            # print(\"Creating 'torque_times_temp' feature...\") # Silence this\n",
        "            df_transformed['torque_times_temp'] = df_transformed['torque_meas'] * df_transformed['outside_air_temp']\n",
        "            # print(\"Dropping 'torque_meas' and 'outside_air_temp' columns...\") # Silence this\n",
        "            df_transformed = df_transformed.drop(columns=['torque_meas', 'outside_air_temp'])\n",
        "\n",
        "        return df_transformed"
      ],
      "metadata": {
        "id": "Otg3kev41Mx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## torque_meas e mean_gas_temp"
      ],
      "metadata": {
        "id": "SPqsslfb6Re4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crea una classe che sia compatibile con l'oggetto pipeline di scikit che,  come nella cella sopra ,faccia il prodotto dalle feature 'torque_meas' e mean_gas_temp' , e crei  una nuova feature 'torque_times_temp', droppa poi le feature ''torque_meas' e 'mean_gas_temp\n",
        "\n",
        "\n",
        "class TorqueTempFeature2(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        required_cols = ['torque_meas', 'mean_gas_temp']\n",
        "        missing_cols = [col for col in required_cols if col not in X.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"The following required columns for TorqueTempFeature were not found in the DataFrame: {missing_cols}\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
        "\n",
        "        df_transformed = X.copy()\n",
        "\n",
        "        required_cols = ['torque_meas', 'mean_gas_temp']\n",
        "        missing_cols = [col for col in required_cols if col not in df_transformed.columns]\n",
        "\n",
        "        if missing_cols:\n",
        "             # print(f\"Warning: Skipping 'torque_times_temp' creation as columns are missing: {missing_cols}\") # Silence this\n",
        "             return df_transformed\n",
        "        else:\n",
        "            # print(\"Creating 'torque_times_temp' feature...\") # Silence this\n",
        "            df_transformed['torque_times_temp'] = df_transformed['torque_meas'] * df_transformed['mean_gas_temp']\n",
        "            # print(\"Dropping 'torque_meas' and 'mean_gas_temp' columns...\") # Silence this\n",
        "            df_transformed = df_transformed.drop(columns=['torque_meas', 'mean_gas_temp'])\n",
        "\n",
        "        return df_transformed"
      ],
      "metadata": {
        "id": "M28rZmTesmad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Separo y_target da test set e train set"
      ],
      "metadata": {
        "id": "VqJ9Muo79Ffr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: estrai dal dataframe 'data_train' 'helath state' e salvalo in data_train_y e droppalo in data_train\n",
        "\n",
        "data_train_y = data_train['y_target']\n",
        "data_train_x = data_train.drop('y_target', axis=1)\n",
        "\n",
        "data_test_y = data_test['y_target']\n",
        "data_test_x = data_test.drop('y_target', axis=1)\n",
        "\n",
        "print(\"data_train after dropping 'y_target':\")\n",
        "print(data_train.head())\n",
        "print(\"\\ndata_train_y (extracted 'y_target'):\")\n",
        "print(data_train_y.head())\n"
      ],
      "metadata": {
        "id": "0wTAar567nkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creazione Pipeline"
      ],
      "metadata": {
        "id": "4_lzYxKpA1Yp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prima Pipeline"
      ],
      "metadata": {
        "id": "xQFom5pu4QRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.compose import make_column_selector, make_column_transformer\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import make_column_selector, make_column_transformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Update the pipeline to include the new transformations\n",
        "preprocessing = Pipeline([\n",
        "    ('rename_features', FeatureRenamer(column_names=new_names_for_train_data[:-1])), # Exclude 'health_state' if not in data_train_x\n",
        "    ('custom_root_power_avail', CustomRootTransformer(root_transformations={'power_avail': 2.35})),\n",
        "    ('binned_air_speed', BinnedQualitativeTransformer(binning_config={'indicated_air_speed': (5, 'quantile')})),\n",
        "    ('pca_speed_power', PCATransformer(columns_to_pca=['net_power', 'compressor_speed'], n_components=1)),\n",
        "    ('torque_temp_feature', TorqueTempFeature()),\n",
        "    ('drop_columns', ColumnDropper(columns_to_drop=['indicated_air_speed', 'power_avail'])), # Add ColumnDropper here\n",
        "    ('scale_data', ColumnStandardizer(columns_to_standardize=[\n",
        "        'mean_gas_temp',\n",
        "        'pca_net_power_compressor_speed',\n",
        "        'torque_times_temp',\n",
        "        'power_avail_root_2.35_transformed'])),\n",
        "  # Assuming you still want scaling at the end\n",
        "])\n",
        "\n",
        "# Apply the updated pipeline to data_train_x\n",
        "data_train_processed_x = preprocessing.fit_transform(data_train_x)\n",
        "\n",
        "data_train_processed_x"
      ],
      "metadata": {
        "id": "NVOQuoySoauN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applicando anche il Onehotencoding"
      ],
      "metadata": {
        "id": "nlVl1KSiQxY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: prendi esempio dalla cella sopra m, ma esegui anche one_hot_encoding su indicated_air_speed_binned_5_quantile\n",
        "\n",
        "import pandas as pd\n",
        "# Update the pipeline to include the new transformations and OneHotEncoder\n",
        "preprocessing_OH = Pipeline([\n",
        "    ('rename_features', FeatureRenamer(column_names=new_names_for_train_data[:-1])), # Exclude 'health_state' if not in data_train_x\n",
        "    ('custom_root_power_avail', CustomRootTransformer(root_transformations={'power_avail': 2.35})),\n",
        "    ('binned_air_speed', BinnedQualitativeTransformer(binning_config={'indicated_air_speed': (5, 'quantile')})),\n",
        "    ('pca_speed_power', PCATransformer(columns_to_pca=['net_power', 'compressor_speed'], n_components=1)),\n",
        "    ('torque_temp_feature', TorqueTempFeature()),\n",
        "    # The ColumnDropper should now drop the original 'indicated_air_speed' and 'power_avail'\n",
        "    ('drop_columns', ColumnDropper(columns_to_drop=['indicated_air_speed', 'power_avail'])),\n",
        "    # Standardize numerical columns (including the new ones)\n",
        "    # Note: 'indicated_air_speed_binned_5_quantile' is categorical and will be handled by OneHotEncoder\n",
        "    ('scale_data', ColumnStandardizer(columns_to_standardize=[\n",
        "        'mean_gas_temp',\n",
        "        'pca_net_power_compressor_speed',\n",
        "        'torque_times_temp',\n",
        "        'power_avail_root_2.35_transformed'])),\n",
        "    # Apply OneHotEncoder to the binned categorical column\n",
        "    ('one_hot_encode_binned_air_speed', ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ['indicated_air_speed_binned_5_quantile']) # sparse_output=False to get a dense array\n",
        "        ],\n",
        "        remainder='passthrough' # Keep other columns (the scaled numerical ones)\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Apply the updated pipeline to data_train_x\n",
        "# The output of ColumnTransformer is a numpy array when remainder='passthrough'\n",
        "data_train_processed_x_array_OH = preprocessing_OH.fit_transform(data_train_x)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FZnyCMkMPEH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: trasforma i dati di data_train_processed_x_array_OH in un  data_frame con i nomi delle feature corrispondenti non printare\n",
        "\n",
        "import pandas as pd\n",
        "def to_dataframe(preprocessing, data_train_x):\n",
        "\n",
        "\n",
        "  data_train_processed_x_array = preprocessing.fit_transform(data_train_x)\n",
        "\n",
        "\n",
        "  column_transformer_step = preprocessing.named_steps['one_hot_encode_binned_air_speed']\n",
        "\n",
        "\n",
        "  pipeline_before_onehot = Pipeline(preprocessing.steps[:-1])\n",
        "\n",
        "\n",
        "  df_before_onehot = pipeline_before_onehot.fit_transform(data_train_x)\n",
        "\n",
        "\n",
        "  onehot_cols_to_encode = column_transformer_step.transformers[0][2]\n",
        "\n",
        "\n",
        "  remainder_cols = [col for col in df_before_onehot.columns if col not in onehot_cols_to_encode]\n",
        "\n",
        "\n",
        "  fitted_onehot_transformer = column_transformer_step.named_transformers_['onehot']\n",
        "  onehot_feature_names = fitted_onehot_transformer.get_feature_names_out(onehot_cols_to_encode)\n",
        "\n",
        "\n",
        "  all_feature_names = list(onehot_feature_names) + remainder_cols\n",
        "\n",
        "  data_train_processed = pd.DataFrame(data_train_processed_x_array, columns=all_feature_names)\n",
        "\n",
        "  return data_train_processed\n",
        "\n",
        "\n",
        "data_train_processed_OH = to_dataframe(preprocessing_OH, data_train_x)\n",
        "\n",
        "data_train_processed_OH.index = data_train_x.index\n",
        "\n",
        "data_train_processed_OH"
      ],
      "metadata": {
        "id": "ekzgHMcW8OEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tenendo outside_air_temp e torque_meas"
      ],
      "metadata": {
        "id": "OyGuIAldoq_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: prendi esempio dalla cella sopra m, ma esegui anche one_hot_encoding su indicated_air_speed_binned_5_quantile\n",
        "\n",
        "import pandas as pd\n",
        "# Update the pipeline to include the new transformations and OneHotEncoder\n",
        "preprocessing_OH_OT = Pipeline([\n",
        "    ('rename_features', FeatureRenamer(column_names=new_names_for_train_data[:-1])), # Exclude 'health_state' if not in data_train_x\n",
        "    ('custom_root_power_avail', CustomRootTransformer(root_transformations={'power_avail': 2.35})),\n",
        "    ('binned_air_speed', BinnedQualitativeTransformer(binning_config={'indicated_air_speed': (5, 'quantile')})),\n",
        "    ('pca_speed_power', PCATransformer(columns_to_pca=['net_power', 'compressor_speed'], n_components=1)),\n",
        "    #('torque_temp_feature', TorqueTempFeature()),\n",
        "    # The ColumnDropper should now drop the original 'indicated_air_speed' and 'power_avail'\n",
        "    ('drop_columns', ColumnDropper(columns_to_drop=['indicated_air_speed', 'power_avail'])),\n",
        "    # Standardize numerical columns (including the new ones)\n",
        "    # Note: 'indicated_air_speed_binned_5_quantile' is categorical and will be handled by OneHotEncoder\n",
        "    ('scale_data', ColumnStandardizer(columns_to_standardize=[\n",
        "        'mean_gas_temp',\n",
        "        'pca_net_power_compressor_speed',\n",
        "        'outside_air_temp',\n",
        "        'torque_meas',\n",
        "        'power_avail_root_2.35_transformed',\n",
        "      ])),\n",
        "    # Apply OneHotEncoder to the binned categorical column\n",
        "    ('one_hot_encode_binned_air_speed', ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ['indicated_air_speed_binned_5_quantile']) # sparse_output=False to get a dense array\n",
        "        ],\n",
        "        remainder='passthrough' # Keep other columns (the scaled numerical ones)\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Apply the updated pipeline to data_train_x\n",
        "# The output of ColumnTransformer is a numpy array when remainder='passthrough'\n",
        "data_train_processed_x_array_OH_OT = preprocessing_OH_OT.fit_transform(data_train_x)\n",
        "\n"
      ],
      "metadata": {
        "id": "IDu99AK5od6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: trasforma i dati di data_train_processed_x_array_OH_OT in un  data_frame con i nomi delle feature corrispondenti non printare\n",
        "\n",
        "data_train_processed_OH_OT = to_dataframe(preprocessing_OH_OT, data_train_x)\n",
        "\n",
        "data_train_processed_OH_OT.index = data_train_x.index\n",
        "\n",
        "data_train_processed_OH_OT"
      ],
      "metadata": {
        "id": "WHMLxUmw9Fc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantile transformer su power available"
      ],
      "metadata": {
        "id": "11usdGv2byWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: prendi esempio dalla cella sotto ma invece di  CustomRootTransformer usa Quantiletransformer\n",
        "\n",
        "# Update the pipeline to include the new transformations and OneHotEncoder\n",
        "preprocessing_OH_QT = Pipeline([\n",
        "    ('rename_features', FeatureRenamer(column_names=new_names_for_train_data[:-1])), # Exclude 'health_state' if not in data_train_x\n",
        "    #('custom_root_power_avail', CustomRootTransformer(root_transformations={'power_avail': 2.35})), # Remove custom root\n",
        "    ('quantile_power_avail', QuantileTransformerWrapper(columns_to_transform=['power_avail'], output_distribution='normal')), # Add QuantileTransformer\n",
        "    ('binned_air_speed', BinnedQualitativeTransformer(binning_config={'indicated_air_speed': (5, 'quantile')})),\n",
        "    ('pca_speed_power', PCATransformer(columns_to_pca=['net_power', 'compressor_speed'], n_components=1)),\n",
        "    # The ColumnDropper should now drop the original 'indicated_air_speed', 'power_avail', 'torque_meas', and 'mean_gas_temp'\n",
        "    ('drop_columns', ColumnDropper(columns_to_drop=['indicated_air_speed', 'power_avail'])),\n",
        "    # Standardize numerical columns (including the new ones)\n",
        "    # Note: 'indicated_air_speed_binned_5_quantile' is categorical and will be handled by OneHotEncoder\n",
        "    # Note: 'power_avail_quantile_transformed' is now the transformed column\n",
        "    # Note: 'torque_times_temp' is the new combined feature\n",
        "    ('scale_data', ColumnStandardizer(columns_to_standardize=[\n",
        "        'mean_gas_temp', # Dropped\n",
        "        'pca_net_power_compressor_speed',\n",
        "        'outside_air_temp',\n",
        "        'torque_meas', # Dropped\n",
        "        #'power_avail_root_2.35_transformed', # Removed\n",
        "        'power_avail_quantile_transformed', # New transformed column\n",
        "      ])),\n",
        "    # Apply OneHotEncoder to the binned categorical column\n",
        "    ('one_hot_encode_binned_air_speed', ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ['indicated_air_speed_binned_5_quantile']) # sparse_output=False to get a dense array\n",
        "        ],\n",
        "        remainder='passthrough' # Keep other columns (the scaled numerical ones)\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Apply the updated pipeline to data_train_x\n",
        "# The output of ColumnTransformer is a numpy array when remainder='passthrough'\n",
        "data_train_processed_x_array_OH_QT = preprocessing_OH_QT.fit_transform(data_train_x)\n"
      ],
      "metadata": {
        "id": "mBrgD3WccCSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: trasforma i dati di data_train_processed_x_array_OH_OT in un  data_frame con i nomi delle feature corrispondenti non printare\n",
        "\n",
        "data_train_processed_OH_QT = to_dataframe(preprocessing_OH_QT, data_train_x)\n",
        "\n",
        "\n",
        "data_train_processed_OH_QT.index = data_train_x.index\n",
        "\n",
        "\n",
        "data_train_processed_OH_QT"
      ],
      "metadata": {
        "id": "ImzMgb9IeCuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature engineering of mean_temp e torq"
      ],
      "metadata": {
        "id": "sCm1lZF7oePr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: prendi esempio dalla cella sopra m, ma esegui anche one_hot_encoding su indicated_air_speed_binned_5_quantile\n",
        "\n",
        "import pandas as pd\n",
        "# Update the pipeline to include the new transformations and OneHotEncoder\n",
        "preprocessing_OH_CH = Pipeline([\n",
        "    ('rename_features', FeatureRenamer(column_names=new_names_for_train_data[:-1])), # Exclude 'health_state' if not in data_train_x\n",
        "    ('quantile_power_avail', QuantileTransformerWrapper(columns_to_transform=['power_avail'], output_distribution='normal')),\n",
        "    ('binned_air_speed', BinnedQualitativeTransformer(binning_config={'indicated_air_speed': (5, 'quantile')})),\n",
        "    ('pca_speed_power', PCATransformer(columns_to_pca=['net_power', 'compressor_speed'], n_components=1)),\n",
        "    ('torque_temp_feature', TorqueTempFeature2()),\n",
        "    # The ColumnDropper should now drop the original 'indicated_air_speed' and 'power_avail'\n",
        "    ('drop_columns', ColumnDropper(columns_to_drop=['indicated_air_speed', 'power_avail'])),\n",
        "    # Standardize numerical columns (including the new ones)\n",
        "    # Note: 'indicated_air_speed_binned_5_quantile' is categorical and will be handled by OneHotEncoder\n",
        "    ('scale_data', ColumnStandardizer(columns_to_standardize=[\n",
        "        'outside_air_temp',\n",
        "        'pca_net_power_compressor_speed',\n",
        "        'torque_times_temp',\n",
        "        'power_avail_quantile_transformed'])),\n",
        "    # Apply OneHotEncoder to the binned categorical column\n",
        "    ('one_hot_encode_binned_air_speed', ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ['indicated_air_speed_binned_5_quantile']) # sparse_output=False to get a dense array\n",
        "        ],\n",
        "        remainder='passthrough' # Keep other columns (the scaled numerical ones)\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Apply the updated pipeline to data_train_x\n",
        "# The output of ColumnTransformer is a numpy array when remainder='passthrough'\n",
        "data_train_processed_x_array_OH_CH = preprocessing_OH_CH.fit_transform(data_train_x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iGYSuRstoNNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: trasforma i dati di data_train_processed_x_array_OH_OT in un  data_frame con i nomi delle feature corrispondenti non printare\n",
        "\n",
        "data_train_processed_OH_CH = to_dataframe(preprocessing_OH_CH, data_train_x)\n",
        "\n",
        "data_train_processed_OH_CH.index = data_train_x.index\n",
        "\n",
        "data_train_processed_OH_CH"
      ],
      "metadata": {
        "id": "WnPA_tb6ojQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Due quantile"
      ],
      "metadata": {
        "id": "9yrG56_SiIcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: prendi esempio dalla cella sotto ma invece di  CustomRootTransformer usa Quantiletransformer\n",
        "\n",
        "# Update the pipeline to include the new transformations and OneHotEncoder\n",
        "preprocessing_OH_AQT = Pipeline([\n",
        "    ('rename_features', FeatureRenamer(column_names=new_names_for_train_data[:-1])), # Exclude 'health_state' if not in data_train_x\n",
        "    #('custom_root_power_avail', CustomRootTransformer(root_transformations={'power_avail': 2.35})), # Remove custom root\n",
        "    ('quantile_power_avail', QuantileTransformerWrapper(columns_to_transform=['power_avail','torque_meas'], output_distribution='normal')), # Add QuantileTransformer\n",
        "    ('binned_air_speed', BinnedQualitativeTransformer(binning_config={'indicated_air_speed': (5, 'quantile')})),\n",
        "    ('pca_speed_power', PCATransformer(columns_to_pca=['net_power', 'compressor_speed'], n_components=1)),\n",
        "    # The ColumnDropper should now drop the original 'indicated_air_speed', 'power_avail', 'torque_meas', and 'mean_gas_temp'\n",
        "    ('drop_columns', ColumnDropper(columns_to_drop=['indicated_air_speed', 'power_avail','torque_meas'])),\n",
        "    # Standardize numerical columns (including the new ones)\n",
        "    # Note: 'indicated_air_speed_binned_5_quantile' is categorical and will be handled by OneHotEncoder\n",
        "    # Note: 'power_avail_quantile_transformed' is now the transformed column\n",
        "    # Note: 'torque_times_temp' is the new combined feature\n",
        "    ('scale_data', ColumnStandardizer(columns_to_standardize=[\n",
        "        'mean_gas_temp',\n",
        "        'pca_net_power_compressor_speed',\n",
        "        'outside_air_temp',\n",
        "        'torque_meas_quantile_transformed', # Dropped\n",
        "        #'power_avail_root_2.35_transformed', # Removed\n",
        "        'power_avail_quantile_transformed', # New transformed column\n",
        "      ])),\n",
        "    # Apply OneHotEncoder to the binned categorical column\n",
        "    ('one_hot_encode_binned_air_speed', ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ['indicated_air_speed_binned_5_quantile']) # sparse_output=False to get a dense array\n",
        "        ],\n",
        "        remainder='passthrough' # Keep other columns (the scaled numerical ones)\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Apply the updated pipeline to data_train_x\n",
        "# The output of ColumnTransformer is a numpy array when remainder='passthrough'\n",
        "data_train_processed_x_array_OH_AQT = preprocessing_OH_QT.fit_transform(data_train_x)\n"
      ],
      "metadata": {
        "id": "xh1ZXmFTh_pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: trasforma i dati di data_train_processed_x_array_OH_OT in un  data_frame con i nomi delle feature corrispondenti non printare\n",
        "\n",
        "data_train_processed_OH_AQT = to_dataframe(preprocessing_OH_AQT, data_train_x)\n",
        "\n",
        "\n",
        "data_train_processed_OH_AQT.index = data_train_x.index\n",
        "\n",
        "\n",
        "data_train_processed_OH_AQT"
      ],
      "metadata": {
        "id": "H25VRhHziFoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jisDWnsWGPji"
      },
      "source": [
        "# Visualizzazione delle pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ARNaif8GPji"
      },
      "source": [
        "## Training and Evaluating on the Training Set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Senza onehotencoding"
      ],
      "metadata": {
        "id": "VmK3o8KGRhNV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlbAwzPYGPji"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_reg = make_pipeline(preprocessing, LogisticRegression())\n",
        "log_reg.fit(data_train_x, data_train_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One_hot_encoding"
      ],
      "metadata": {
        "id": "FyvcvNoWrlqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_reg_OH = make_pipeline(preprocessing_OH, LogisticRegression())\n",
        "log_reg_OH.fit(data_train_x, data_train_y)"
      ],
      "metadata": {
        "id": "mY44v-IdUV7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tenendo outside air temp e torque_meas"
      ],
      "metadata": {
        "id": "u1KS9UlQreeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_reg_OH_OT = make_pipeline(preprocessing_OH_OT, LogisticRegression())\n",
        "log_reg_OH_OT.fit(data_train_x, data_train_y)"
      ],
      "metadata": {
        "id": "CHzXEMqip2QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantile transformation su power_avail"
      ],
      "metadata": {
        "id": "JtoaBJkhfsaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_reg_OH_QT = make_pipeline(preprocessing_OH_QT, LogisticRegression())\n",
        "log_reg_OH_QT.fit(data_train_x, data_train_y)"
      ],
      "metadata": {
        "id": "w0BqnU5yfw4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature engineering mean temp e  torq"
      ],
      "metadata": {
        "id": "95PaDr5OpQEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_reg_OH_CH = make_pipeline(preprocessing_OH_CH, LogisticRegression())\n",
        "log_reg_OH_CH.fit(data_train_x, data_train_y)"
      ],
      "metadata": {
        "id": "I534LmFEo6nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Due quantile"
      ],
      "metadata": {
        "id": "jfm2dSL8itGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_reg_OH_AQT = make_pipeline(preprocessing_OH_AQT, LogisticRegression())\n",
        "log_reg_OH_AQT.fit(data_train_x, data_train_y)"
      ],
      "metadata": {
        "id": "z0KBhAVOiklz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Valutazione performance al variare delle trasformazione nella pipeline"
      ],
      "metadata": {
        "id": "3637iPNwyFQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Senza onehotencoding"
      ],
      "metadata": {
        "id": "qd0SOU3mZA_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: calcola l'accurcay, precision, sensitivity e specificity della regressione logistica dopo aver elaborato i dati tramite pipeline\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = log_reg.predict(data_test_x)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(data_test_y, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(data_test_y, y_pred)\n",
        "precision = precision_score(data_test_y, y_pred)\n",
        "sensitivity = recall_score(data_test_y, y_pred) # Sensitivity is also known as Recall\n",
        "# Specificity: TN / (TN + FP)\n",
        "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0 # Handle potential division by zero\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
        "print(f\"Specificity: {specificity:.4f}\")\n"
      ],
      "metadata": {
        "id": "FApju7mDyKaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noto che le prestazioni sono basse, quindi cerco di agire sulla pipeline per migliorarle."
      ],
      "metadata": {
        "id": "BJAQJ4k74Y9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Con onehotencoding"
      ],
      "metadata": {
        "id": "GOxQQf8mZEr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: calcola l'accurcay, precision, sensitivity e specificity della regressione logistica dopo aver elaborato i dati tramite pipeline\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = log_reg_OH.predict(data_test_x)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(data_test_y, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(data_test_y, y_pred)\n",
        "precision = precision_score(data_test_y, y_pred)\n",
        "sensitivity = recall_score(data_test_y, y_pred) # Sensitivity is also known as Recall\n",
        "# Specificity: TN / (TN + FP)\n",
        "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0 # Handle potential division by zero\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
        "print(f\"Specificity: {specificity:.4f}\")"
      ],
      "metadata": {
        "id": "Ja75Hdi3UqdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Noto un miglioramento su tutti gli  indici. È opportuno utilzzare onehotencoding"
      ],
      "metadata": {
        "id": "8P7r10YuUxAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tenendo outside_air_temp e torque_meas"
      ],
      "metadata": {
        "id": "YxHEpAk_p9Q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: calcola l'accurcay, precision, sensitivity e specificity della regressione logistica dopo aver elaborato i dati tramite pipeline\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = log_reg_OH_OT.predict(data_test_x)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(data_test_y, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(data_test_y, y_pred)\n",
        "precision = precision_score(data_test_y, y_pred)\n",
        "sensitivity = recall_score(data_test_y, y_pred) # Sensitivity is also known as Recall\n",
        "# Specificity: TN / (TN + FP)\n",
        "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0 # Handle potential division by zero\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
        "print(f\"Specificity: {specificity:.4f}\")"
      ],
      "metadata": {
        "id": "1fQUOL4Ep881"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Non utilizzando la feature engineering su outside_air_temp e torque_meas si registra un migliroamento delle prestazioni."
      ],
      "metadata": {
        "id": "p7kDeph72v9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantile transformation su power_avail"
      ],
      "metadata": {
        "id": "AprsAk9mRXfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: calcola l'accurcay, precision, sensitivity e specificity della regressione logistica dopo aver elaborato i dati tramite pipeline\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = log_reg_OH_QT.predict(data_test_x)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(data_test_y, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(data_test_y, y_pred)\n",
        "precision = precision_score(data_test_y, y_pred)\n",
        "sensitivity = recall_score(data_test_y, y_pred) # Sensitivity is also known as Recall\n",
        "# Specificity: TN / (TN + FP)\n",
        "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0 # Handle potential division by zero\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
        "print(f\"Specificity: {specificity:.4f}\")"
      ],
      "metadata": {
        "id": "MVtU5Nb3f4yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noto un ulteriore miglioramento delle prestazioni a seguito del cambio del tipo di trasformazione effettuata su power_avail."
      ],
      "metadata": {
        "id": "j7nrTyWn29Mt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature engineering di mean_temp e torq"
      ],
      "metadata": {
        "id": "yKVRmq7lpEwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: calcola l'accurcay, precision, sensitivity e specificity della regressione logistica dopo aver elaborato i dati tramite pipeline\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = log_reg_OH_CH.predict(data_test_x)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(data_test_y, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(data_test_y, y_pred)\n",
        "precision = precision_score(data_test_y, y_pred)\n",
        "sensitivity = recall_score(data_test_y, y_pred) # Sensitivity is also known as Recall\n",
        "# Specificity: TN / (TN + FP)\n",
        "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0 # Handle potential division by zero\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
        "print(f\"Specificity: {specificity:.4f}\")"
      ],
      "metadata": {
        "id": "TyWvkn35pBNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noto un peggioramento dei vari indici a seguito dell'applicazione di una feature engineering alternativa."
      ],
      "metadata": {
        "id": "E_V_CIONf_NO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: calcola l'accurcay, precision, sensitivity e specificity della regressione logistica dopo aver elaborato i dati tramite pipeline\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = log_reg_OH_AQT.predict(data_test_x)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(data_test_y, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(data_test_y, y_pred)\n",
        "precision = precision_score(data_test_y, y_pred)\n",
        "sensitivity = recall_score(data_test_y, y_pred) # Sensitivity is also known as Recall\n",
        "# Specificity: TN / (TN + FP)\n",
        "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0 # Handle potential division by zero\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
        "print(f\"Specificity: {specificity:.4f}\")"
      ],
      "metadata": {
        "id": "HielCwn2iy44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analisi dei risultati:"
      ],
      "metadata": {
        "id": "QghkSBobt1zz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La trasformazione che è stata utilizzata in precedenza, che prevedeva la moltiplicazione di 'outside_air_temp' e 'torque_meas', non è efficace. Al contrario, ha avuto un esito positivo l'applicazione dello onehotencoding e il cambio di trasformazione per power_avail."
      ],
      "metadata": {
        "id": "d308LZvbt7kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisi delle prestazioni del quarto modello"
      ],
      "metadata": {
        "id": "Kn1x16dxJXCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross validation"
      ],
      "metadata": {
        "id": "3aYzKylQYo8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: utilizza il cross_val_score per valutaeìre le performance di log_reg_OH_QT\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Evaluate the log_reg_OH_QT pipeline using cross-validation\n",
        "# We use data_train_x and data_train_y for evaluation with cross-validation,\n",
        "# as cross_val_score handles the splitting internally.\n",
        "# Common scoring metrics for classification are 'accuracy', 'precision', 'recall', 'f1'.\n",
        "# Let's evaluate using accuracy as an example.\n",
        "\n",
        "# Use the pipeline log_reg_OH_QT directly in cross_val_score\n",
        "# cross_val_score takes the estimator (your pipeline), the data, and the target.\n",
        "# cv=... specifies the number of folds for cross-validation.\n",
        "# scoring='accuracy' specifies the metric to use.\n",
        "log_reg_OH_QT_scores_accuracy = cross_val_score(log_reg_OH_QT, data_train_x, data_train_y, cv=10, scoring=\"accuracy\")\n",
        "\n",
        "print(\"Cross-validation Accuracy Scores (log_reg_OH_QT):\", log_reg_OH_QT_scores_accuracy)\n",
        "print(\"Mean CV Accuracy (log_reg_OH_QT):\", log_reg_OH_QT_scores_accuracy.mean())\n",
        "print(\"Standard Deviation of CV Accuracy (log_reg_OH_QT):\", log_reg_OH_QT_scores_accuracy.std())\n",
        "\n",
        "# You can evaluate other metrics as well:\n",
        "log_reg_OH_QT_scores_precision = cross_val_score(log_reg_OH_QT, data_train_x, data_train_y, cv=10, scoring=\"precision\")\n",
        "print(\"\\nCross-validation Precision Scores (log_reg_OH_QT):\", log_reg_OH_QT_scores_precision)\n",
        "print(\"Mean CV Precision (log_reg_OH_QT):\", log_reg_OH_QT_scores_precision.mean())\n",
        "\n",
        "log_reg_OH_QT_scores_recall = cross_val_score(log_reg_OH_QT, data_train_x, data_train_y, cv=10, scoring=\"recall\")\n",
        "print(\"\\nCross-validation Recall (Sensitivity) Scores (log_reg_OH_QT):\", log_reg_OH_QT_scores_recall)\n",
        "print(\"Mean CV Recall (log_reg_OH_QT):\", log_reg_OH_QT_scores_recall.mean())\n",
        "\n",
        "log_reg_OH_QT_scores_f1 = cross_val_score(log_reg_OH_QT, data_train_x, data_train_y, cv=10, scoring=\"f1\")\n",
        "print(\"\\nCross-validation F1 Scores (log_reg_OH_QT):\", log_reg_OH_QT_scores_f1)\n",
        "print(\"Mean CV F1 (log_reg_OH_QT):\", log_reg_OH_QT_scores_f1.mean())\n"
      ],
      "metadata": {
        "id": "zJZ2r_vJXsmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Appliccando il cross-validation si ottengno degli indici pressochè identici per tutte le fold."
      ],
      "metadata": {
        "id": "wNXaGCA84IJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: utilizza il cross_val_score per valutaeìre le performance di log_reg_OH_AQT\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Evaluate the log_reg_OH_AQT pipeline using cross-validation\n",
        "# We use data_train_x and data_train_y for evaluation with cross-validation,\n",
        "# as cross_val_score handles the splitting internally.\n",
        "# Common scoring metrics for classification are 'accuracy', 'precision', 'recall', 'f1'.\n",
        "# Let's evaluate using accuracy as an example.\n",
        "\n",
        "# Use the pipeline log_reg_OH_AQT directly in cross_val_score\n",
        "# cross_val_score takes the estimator (your pipeline), the data, and the target.\n",
        "# cv=... specifies the number of folds for cross-validation.\n",
        "# scoring='accuracy' specifies the metric to use.\n",
        "log_reg_OH_AQT_scores_accuracy = cross_val_score(log_reg_OH_AQT, data_train_x, data_train_y, cv=10, scoring=\"accuracy\")\n",
        "\n",
        "print(\"Cross-validation Accuracy Scores (log_reg_OH_AQT):\", log_reg_OH_AQT_scores_accuracy)\n",
        "print(\"Mean CV Accuracy (log_reg_OH_AQT):\", log_reg_OH_AQT_scores_accuracy.mean())\n",
        "print(\"Standard Deviation of CV Accuracy (log_reg_OH_AQT):\", log_reg_OH_AQT_scores_accuracy.std())\n",
        "\n",
        "# You can evaluate other metrics as well:\n",
        "log_reg_OH_AQT_scores_precision = cross_val_score(log_reg_OH_AQT, data_train_x, data_train_y, cv=10, scoring=\"precision\")\n",
        "print(\"\\nCross-validation Precision Scores (log_reg_OH_AQT):\", log_reg_OH_AQT_scores_precision)\n",
        "print(\"Mean CV Precision (log_reg_OH_AQT):\", log_reg_OH_AQT_scores_precision.mean())\n",
        "\n",
        "log_reg_OH_AQT_scores_recall = cross_val_score(log_reg_OH_AQT, data_train_x, data_train_y, cv=10, scoring=\"recall\")\n",
        "print(\"\\nCross-validation Recall (Sensitivity) Scores (log_reg_OH_AQT):\", log_reg_OH_AQT_scores_recall)\n",
        "print(\"Mean CV Recall (log_reg_OH_AQT):\", log_reg_OH_AQT_scores_recall.mean())\n",
        "\n",
        "log_reg_OH_AQT_scores_f1 = cross_val_score(log_reg_OH_AQT, data_train_x, data_train_y, cv=10, scoring=\"f1\")\n",
        "print(\"\\nCross-validation F1 Scores (log_reg_OH_AQT):\", log_reg_OH_AQT_scores_f1)\n",
        "print(\"Mean CV F1 (log_reg_OH_AQT):\", log_reg_OH_AQT_scores_f1.mean())\n"
      ],
      "metadata": {
        "id": "i2pqDQLyi97A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_reg_OPT= log_reg_OH_QT\n",
        "preprocessing_OPT = preprocessing_OH_QT\n",
        "data_train_processed_OPT = data_train_processed_OH_QT"
      ],
      "metadata": {
        "id": "SfpScvsFqWIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion matrix"
      ],
      "metadata": {
        "id": "iILVcHNpPNdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: costruisci una confusion matrix di log_reg_OH con i colori\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Make predictions on the test set using the log_reg_OH pipeline\n",
        "y_pred_OPT = log_reg_OPT.predict(data_test_x)\n",
        "\n",
        "# Calculate the confusion matrix for log_reg_OH\n",
        "cm_OPT = confusion_matrix(data_test_y, y_pred_OPT)\n",
        "\n",
        "# Plot the confusion matrix with colors\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_OPT, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "            xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion Matrix for Logistic Regression with One-Hot Encoding')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KvZSfPhiO2eE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analisi coefficienti"
      ],
      "metadata": {
        "id": "p0kTpB5cPRJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: definisci all_processed_feature_names come i nomi dei dati trasformati con la pipeline\n",
        "\n",
        "all_processed_feature_names = data_train_processed_OPT.columns.tolist()\n",
        "\n",
        "print(\"Nomi delle feature trasformate dalla pipeline OPT:\")\n",
        "all_processed_feature_names"
      ],
      "metadata": {
        "id": "uGQ5egilq0dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: printa i coefficienti della regressione logistica log_reg_OPT e le feature corrispondenti\n",
        "\n",
        "# Get the Logistic Regression model from the pipeline\n",
        "log_reg_model_OPT = log_reg_OPT.named_steps['logisticregression']\n",
        "\n",
        "# Get the coefficients\n",
        "coefficients_OPT = log_reg_model_OPT.coef_[0]\n",
        "\n",
        "# Get the intercept\n",
        "intercept_OPT = log_reg_model_OPT.intercept_[0]\n",
        "\n",
        "# Print the coefficients and corresponding feature names\n",
        "print(\"Intercept:\", intercept_OPT)\n",
        "print(\"\\nCoefficients:\")\n",
        "for feature, coef in zip(all_processed_feature_names, coefficients_OPT):\n",
        "    print(f\"{feature}: {coef:.4f}\")"
      ],
      "metadata": {
        "id": "-ojwfF2IZe38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dall'analisi dei coefficienti si evince come le feature che hanno un impatto maggiore sull'output sono 'mean_gas_temp' e 'torque_meas'."
      ],
      "metadata": {
        "id": "pRK3-u9UOnR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### p value"
      ],
      "metadata": {
        "id": "WR7QgedgYXWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: # prompt: analizza SOLO i p value associati a log_reg_OPT NON odds ratio\n",
        "\n",
        "!pip install statsmodels\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Assicurati di avere i dati preprocessati per log_reg_OPT\n",
        "# Questi dati sono stati creati in precedenza e sono in data_train_processed_OPT\n",
        "# E la variabile target data_train_y\n",
        "\n",
        "# Aggiungi una costante (intercetta) ai dati delle feature per statsmodels\n",
        "X_train_sm_OPT = sm.add_constant(data_train_processed_OPT)\n",
        "\n",
        "# Fit il modello di regressione logistica con statsmodels\n",
        "logit_model_OPT = sm.Logit(data_train_y, X_train_sm_OPT)\n",
        "result_OPT = logit_model_OPT.fit()\n",
        "\n",
        "# Stampa il summary del modello per vedere i p-value\n",
        "print(result_OPT.summary())\n",
        "\n",
        "# Estrai i p-value\n",
        "p_values_OPT = result_OPT.pvalues\n",
        "\n",
        "print(\"\\nP-values per log_reg_OPT:\")\n",
        "p_values_OPT\n"
      ],
      "metadata": {
        "id": "KcvNHUFO5d4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dato che tutte le feature numeriche hanno un p-value minore di 0.05, c'è correlazione con l'output."
      ],
      "metadata": {
        "id": "NxKpA7pr7Aqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: analizza l'odds ratio associati a log_reg_OPT\n",
        "\n",
        "import numpy as np\n",
        "# Analisi dell'Odds Ratio per log_reg_OPT\n",
        "\n",
        "# Get the Logistic Regression model from the pipeline\n",
        "log_reg_model_OPT = log_reg_OPT.named_steps['logisticregression']\n",
        "\n",
        "# Get the coefficients\n",
        "coefficients_OPT = log_reg_model_OPT.coef_[0]\n",
        "\n",
        "# Get the feature names from the processed data\n",
        "# This uses the function defined previously to get the correct feature names\n",
        "all_processed_feature_names = to_dataframe(preprocessing_OPT, data_train_x).columns.tolist()\n",
        "\n",
        "# Calculate Odds Ratios (exp(coefficient))\n",
        "odds_ratios_OPT = np.exp(coefficients_OPT)\n",
        "\n",
        "print(\"\\nOdds Ratios (log_reg_OPT):\")\n",
        "for feature, odds in zip(all_processed_feature_names, odds_ratios_OPT):\n",
        "    print(f\"{feature}: {odds:.4f}\")\n",
        "\n",
        "# Note: Calculating p-values for individual coefficients in scikit-learn's\n",
        "# LogisticRegression is not straightforward. scikit-learn is designed for\n",
        "# prediction and performance, not statistical inference.\n",
        "# To get p-values and more detailed statistical summaries (like those in statsmodels),\n",
        "# you would typically need to use a library like statsmodels.\n",
        "\n",
        "# If you needed to calculate p-values, you would generally do the following\n",
        "# (but this requires fitting the model again using statsmodels):\n",
        "\n",
        "# import statsmodels.api as sm\n",
        "#\n",
        "# # Get the processed training data (as a DataFrame from to_dataframe)\n",
        "# data_train_processed_OPT_sm = to_dataframe(preprocessing_OPT, data_train_x)\n",
        "#\n",
        "# # Add a constant (intercept) term for statsmodels\n",
        "# X_sm = sm.add_constant(data_train_processed_OPT_sm)\n",
        "#\n",
        "# # Fit the logistic regression model using statsmodels\n",
        "# # Ensure y_target is the correct target variable (0 or 1)\n",
        "# sm_log_reg_model = sm.Logit(data_train_y, X_sm).fit()\n",
        "#\n",
        "# # Print the summary which includes p-values\n",
        "# print(\"\\nStatsmodels Logistic Regression Summary:\")\n",
        "# print(sm_log_reg_model.summary())\n",
        "\n",
        "# For this response, we focus on the Odds Ratios as requested and based on the\n",
        "# available scikit-learn model fit. We'll mention the limitation regarding p-values.\n",
        "\n",
        "print(\"\\nAnalysis of Odds Ratios for log_reg_OPT:\")\n",
        "print(\"The Odds Ratio for a feature represents the change in the odds of the positive class (y=1) for a one-unit increase in that feature, holding all other features constant.\")\n",
        "print(\"- If Odds Ratio > 1: The odds of the positive class increase as the feature value increases.\")\n",
        "print(\"- If Odds Ratio < 1: The odds of the positive class decrease as the feature value increases.\")\n",
        "print(\"- If Odds Ratio ≈ 1: The feature has little effect on the odds.\")\n",
        "print(\"\\nNote: Direct p-value calculation for coefficients is not directly available in scikit-learn's LogisticRegression. A library like statsmodels would be needed for this statistical inference.\")\n"
      ],
      "metadata": {
        "id": "2pK2jKslMVkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dall'analisi del p-value risulta come le 4 feature numeriche risultino correlate all'uscita (p-value inferiore a 0.05)."
      ],
      "metadata": {
        "id": "pv2TUdwcXMGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "C'è una correlazione molto forte con power_available"
      ],
      "metadata": {
        "id": "u-xYVQSrYabO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Curva ROC"
      ],
      "metadata": {
        "id": "SaXsasLIP7pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: plotta la curva AUC\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Get the predicted probabilities for the positive class (class 1)\n",
        "y_prob = log_reg_OPT.predict_proba(data_test_x)[:, 1]\n",
        "\n",
        "# Calculate the false positive rate (fpr) and true positive rate (tpr) for various thresholds\n",
        "fpr, tpr, thresholds = roc_curve(data_test_y, y_prob)\n",
        "\n",
        "# Calculate the Area Under the Curve (AUC)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing') # Diagonal line\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Area Under the ROC Curve (AUC): {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "7yIiXCzSPT3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'indice AUC della curva ROC è elevato."
      ],
      "metadata": {
        "id": "zo6goD715uIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lasso regression"
      ],
      "metadata": {
        "id": "FmXOjscFBnf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: prova ad usare la lasso regression per trovare la funzione di regressione logistica\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "\n",
        "# Define the Logistic Regression model with Lasso (L1) penalty\n",
        "log_reg_lasso_model = LogisticRegression(penalty='l1', solver='liblinear', C=1.0, random_state=42) # C=1.0 is a common starting point\n",
        "\n",
        "# Integrate this model into the pipeline with OneHotEncoding\n",
        "# We will replace the standard LogisticRegression step with the Lasso-regularized one.\n",
        "log_reg_lasso_OPT_pipeline = make_pipeline(preprocessing_OPT, log_reg_lasso_model)\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "# Note: The preprocessing_OH pipeline is assumed to be defined and fitted already based on the preceding code.\n",
        "# If you run this code block independently, make sure preprocessing_OH is available and fitted.\n",
        "log_reg_lasso_OPT_pipeline.fit(data_train_x, data_train_y)\n",
        "\n",
        "print(\"Lasso-regularized Logistic Regression pipeline fitted.\")\n",
        "\n",
        "# You can now use log_reg_lasso_OPT_pipeline for predictions and evaluation\n",
        "# Make predictions on the test set\n",
        "y_pred_lasso = log_reg_lasso_OPT_pipeline.predict(data_test_x)\n",
        "\n",
        "# Evaluate the model (using confusion matrix, accuracy, etc. as done previously)\n",
        "cm_lasso = confusion_matrix(data_test_y, y_pred_lasso)\n",
        "tn_lasso, fp_lasso, fn_lasso, tp_lasso = cm_lasso.ravel()\n",
        "\n",
        "accuracy_lasso = accuracy_score(data_test_y, y_pred_lasso)\n",
        "precision_lasso = precision_score(data_test_y, y_pred_lasso)\n",
        "sensitivity_lasso = recall_score(data_test_y, y_pred_lasso)\n",
        "specificity_lasso = tn_lasso / (tn_lasso + fp_lasso) if (tn_lasso + fp_lasso) > 0 else 0\n",
        "\n",
        "print(\"\\nEvaluation Metrics for Lasso-regularized Logistic Regression:\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_lasso)\n",
        "print(f\"\\nAccuracy: {accuracy_lasso:.4f}\")\n",
        "print(f\"Precision: {precision_lasso:.4f}\")\n",
        "print(f\"Sensitivity (Recall): {sensitivity_lasso:.4f}\")\n",
        "print(f\"Specificity: {specificity_lasso:.4f}\")\n",
        "\n",
        "# Analyze coefficients (Lasso regularization tends to push some coefficients to exactly zero)\n",
        "log_reg_lasso_model_fitted = log_reg_lasso_OPT_pipeline.named_steps['logisticregression']\n",
        "\n",
        "if hasattr(log_reg_lasso_model_fitted, 'coef_'):\n",
        "    print(\"\\nCoefficients of the Lasso-regularized Logistic Regression Model:\")\n",
        "    print(\"Coefficients:\", log_reg_lasso_model_fitted.coef_[0])\n",
        "    print(\"Intercept:\", log_reg_lasso_model_fitted.intercept_[0])\n",
        "\n",
        "    # To see which coefficient corresponds to which feature\n",
        "    # We need the column names after preprocessing, which are in all_processed_feature_names\n",
        "    if len(log_reg_lasso_model_fitted.coef_[0]) == len(all_processed_feature_names):\n",
        "        coefficients_lasso_df = pd.DataFrame({\n",
        "            'Feature': all_processed_feature_names,\n",
        "            'Coefficient': log_reg_lasso_model_fitted.coef_[0]\n",
        "        })\n",
        "        # Sort by absolute coefficient value to see the most important features\n",
        "        coefficients_lasso_df['Abs_Coefficient'] = np.abs(coefficients_lasso_df['Coefficient'])\n",
        "        coefficients_lasso_df_sorted = coefficients_lasso_df.sort_values(by='Abs_Coefficient', ascending=False).drop(columns='Abs_Coefficient')\n",
        "\n",
        "        print(\"\\nFeature Coefficients (Lasso, Sorted by Absolute Value):\")\n",
        "        print(coefficients_lasso_df_sorted)\n",
        "\n",
        "        # Count how many coefficients are exactly zero\n",
        "        zero_coefficients_count = (coefficients_lasso_df['Coefficient'] == 0).sum()\n",
        "        print(f\"\\nNumber of coefficients exactly zero (due to L1 regularization): {zero_coefficients_count}\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\nWarning: Number of coefficients does not match the number of processed features.\")\n",
        "\n",
        "# Plot ROC Curve for Lasso model\n",
        "y_prob_lasso = log_reg_lasso_OPT_pipeline.predict_proba(data_test_x)[:, 1]\n",
        "fpr_lasso, tpr_lasso, thresholds_lasso = roc_curve(data_test_y, y_prob_lasso)\n",
        "roc_auc_lasso = auc(fpr_lasso, tpr_lasso)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_lasso, tpr_lasso, color='darkorange', lw=2, label=f'ROC curve (Lasso) (AUC = {roc_auc_lasso:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for Lasso-regularized Logistic Regression')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nArea Under the ROC Curve (AUC) for Lasso: {roc_auc_lasso:.4f}\")\n"
      ],
      "metadata": {
        "id": "g21kFFL0BNyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Peggiora le prestazioni."
      ],
      "metadata": {
        "id": "cN1RwbfvIQKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ridge regression"
      ],
      "metadata": {
        "id": "RW4s3XCYH7g0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Applica la ridge regression\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Define the Logistic Regression model with Ridge (L2) penalty\n",
        "log_reg_ridge_model = LogisticRegression(penalty='l2', solver='liblinear', C=1.0, random_state=42) # C=1.0 is a common starting point, solver='liblinear' works with l2\n",
        "\n",
        "# Integrate this model into the pipeline with OneHotEncoding and chosen feature engineering\n",
        "# We will use the preprocessing_OPT pipeline which performed better in the previous analysis.\n",
        "# Replace the standard LogisticRegression step with the Ridge-regularized one.\n",
        "log_reg_ridge_OPT_pipeline = make_pipeline(preprocessing_OPT, log_reg_ridge_model)\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "log_reg_ridge_OPT_pipeline.fit(data_train_x, data_train_y)\n",
        "\n",
        "print(\"Ridge-regularized Logistic Regression pipeline fitted using preprocessing_OPT.\")\n",
        "\n",
        "# You can now use log_reg_ridge_OPT_pipeline for predictions and evaluation\n",
        "# Make predictions on the test set\n",
        "y_pred_ridge = log_reg_ridge_OPT_pipeline.predict(data_test_x)\n",
        "\n",
        "# Evaluate the model (using confusion matrix, accuracy, etc. as done previously)\n",
        "cm_ridge = confusion_matrix(data_test_y, y_pred_ridge)\n",
        "tn_ridge, fp_ridge, fn_ridge, tp_ridge = cm_ridge.ravel()\n",
        "\n",
        "accuracy_ridge = accuracy_score(data_test_y, y_pred_ridge)\n",
        "precision_ridge = precision_score(data_test_y, y_pred_ridge)\n",
        "sensitivity_ridge = recall_score(data_test_y, y_pred_ridge)\n",
        "specificity_ridge = tn_ridge / (tn_ridge + fp_ridge) if (tn_ridge + fp_ridge) > 0 else 0\n",
        "\n",
        "print(\"\\nEvaluation Metrics for Ridge-regularized Logistic Regression:\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_ridge)\n",
        "print(f\"\\nAccuracy: {accuracy_ridge:.4f}\")\n",
        "print(f\"Precision: {precision_ridge:.4f}\")\n",
        "print(f\"Sensitivity (Recall): {sensitivity_ridge:.4f}\")\n",
        "print(f\"Specificity: {specificity_ridge:.4f}\")\n",
        "\n",
        "# Analyze coefficients (Ridge regularization shrinks coefficients towards zero but doesn't make them exactly zero)\n",
        "log_reg_ridge_model_fitted = log_reg_ridge_OPT_pipeline.named_steps['logisticregression']\n",
        "\n",
        "if hasattr(log_reg_ridge_model_fitted, 'coef_'):\n",
        "    print(\"\\nCoefficients of the Ridge-regularized Logistic Regression Model:\")\n",
        "    # Use all_processed_feature_names obtained from the previous cell\n",
        "    if len(log_reg_ridge_model_fitted.coef_[0]) == len(all_processed_feature_names):\n",
        "        coefficients_ridge_df = pd.DataFrame({\n",
        "            'Feature': all_processed_feature_names,\n",
        "            'Coefficient': log_reg_ridge_model_fitted.coef_[0]\n",
        "        })\n",
        "        # Sort by absolute coefficient value\n",
        "        coefficients_ridge_df['Abs_Coefficient'] = np.abs(coefficients_ridge_df['Coefficient'])\n",
        "        coefficients_ridge_df_sorted = coefficients_ridge_df.sort_values(by='Abs_Coefficient', ascending=False).drop(columns='Abs_Coefficient')\n",
        "\n",
        "        print(\"\\nFeature Coefficients (Ridge, Sorted by Absolute Value):\")\n",
        "        print(coefficients_ridge_df_sorted)\n",
        "\n",
        "    else:\n",
        "        print(\"\\nWarning: Number of coefficients does not match the number of processed features.\")\n",
        "        print(\"Number of coefficients:\", len(log_reg_ridge_model_fitted.coef_[0]))\n",
        "        print(\"Number of processed features:\", len(all_processed_feature_names))\n",
        "\n",
        "else:\n",
        "    print(\"The fitted model does not have 'coef_' attribute.\")\n",
        "\n",
        "\n",
        "# Plot ROC Curve for Ridge model\n",
        "y_prob_ridge = log_reg_ridge_OPT_pipeline.predict_proba(data_test_x)[:, 1]\n",
        "fpr_ridge, tpr_ridge, thresholds_ridge = roc_curve(data_test_y, y_prob_ridge)\n",
        "roc_auc_ridge = auc(fpr_ridge, tpr_ridge)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_ridge, tpr_ridge, color='darkgreen', lw=2, label=f'ROC curve (Ridge) (AUC = {roc_auc_ridge:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for Ridge-regularized Logistic Regression')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nArea Under the ROC Curve (AUC) for Ridge: {roc_auc_ridge:.4f}\")\n"
      ],
      "metadata": {
        "id": "MDnCU4BDHpbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sia la ridge che la lasso regression peggiorano le prestazioni. Questo è dovuto al fatto che tutte le feature sono determinanti per predire lo stato di salute."
      ],
      "metadata": {
        "id": "5K24_fVfIUTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analisi risultati"
      ],
      "metadata": {
        "id": "wsl-AzoFxn-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La lasso regression peggiora le performance"
      ],
      "metadata": {
        "id": "_aT8IYGlxsqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning dei parametri"
      ],
      "metadata": {
        "id": "YuNVj3gZbNcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: esegui un'operazione di fine tuning sui parametri di log_reg_OPT\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid to tune for the Logistic Regression step\n",
        "# The parameter names in the grid must be prefixed with the name of the step in the pipeline ('logisticregression')\n",
        "# followed by a double underscore '__' and the parameter name.\n",
        "param_grid = {\n",
        "    'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100], # Inverse of regularization strength; smaller values specify stronger regularization.\n",
        "    'logisticregression__penalty': ['l2'], # Regularization type. 'l1' can also be used, but requires 'liblinear' solver for some versions. 'l2' is standard.\n",
        "    'logisticregression__solver': ['lbfgs', 'liblinear'], # Algorithm to use in the optimization problem. 'liblinear' is good for small datasets, 'lbfgs' is generally faster for larger ones and supports 'l2'.\n",
        "    'logisticregression__max_iter': [1000, 2000, 3000] # Maximum number of iterations for the solver to converge.\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "# estimator: Your pipeline (log_reg_OPT)\n",
        "# param_grid: The dictionary of parameters to tune\n",
        "# cv: Number of cross-validation folds (e.g., 5 or 10)\n",
        "# scoring: The metric to optimize (e.g., 'accuracy', 'precision', 'recall', 'f1', 'roc_auc')\n",
        "# n_jobs: Number of CPU cores to use (-1 means all available cores)\n",
        "# verbose: Controls the verbosity: higher values mean more output during the process.\n",
        "grid_search = GridSearchCV(estimator=log_reg_OPT,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=5, # Using 5 folds for cross-validation\n",
        "                           scoring='roc_auc', # Optimizing for AUC, as it considers the trade-off between sensitivity and specificity\n",
        "                           n_jobs=-1, # Use all available cores\n",
        "                           verbose=2) # Print progress messages\n",
        "\n",
        "# Fit GridSearchCV on the training data (features and target)\n",
        "# GridSearchCV will automatically apply the pipeline's preprocessing steps within each fold.\n",
        "grid_search.fit(data_train_x, data_train_y)\n",
        "\n",
        "# Get the best parameters found by GridSearchCV\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Get the best estimator (the pipeline with the best parameters)\n",
        "best_log_reg_OPT = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred_best = best_log_reg_OPT.predict(data_test_x)\n",
        "y_prob_best = best_log_reg_OPT.predict_proba(data_test_x)[:, 1]\n",
        "\n",
        "# Calculate and print metrics for the best model on the test set\n",
        "cm_best = confusion_matrix(data_test_y, y_pred_best)\n",
        "tn_best, fp_best, fn_best, tp_best = cm_best.ravel()\n",
        "\n",
        "accuracy_best = accuracy_score(data_test_y, y_pred_best)\n",
        "precision_best = precision_score(data_test_y, y_pred_best)\n",
        "sensitivity_best = recall_score(data_test_y, y_pred_best)\n",
        "specificity_best = tn_best / (tn_best + fp_best) if (tn_best + fp_best) > 0 else 0\n",
        "roc_auc_best = auc(roc_curve(data_test_y, y_prob_best)[0], roc_curve(data_test_y, y_prob_best)[1])\n",
        "\n",
        "\n",
        "print(\"\\nMetrics for the Best Model (after GridSearchCV) on the Test Set:\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_best)\n",
        "print(f\"\\nAccuracy: {accuracy_best:.4f}\")\n",
        "print(f\"Precision: {precision_best:.4f}\")\n",
        "print(f\"Sensitivity (Recall): {sensitivity_best:.4f}\")\n",
        "print(f\"Specificity: {specificity_best:.4f}\")\n",
        "print(f\"Area Under the ROC Curve (AUC): {roc_auc_best:.4f}\")\n",
        "\n",
        "# Optional: Plot Confusion Matrix and ROC Curve for the best model\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_best, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "            xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion Matrix for Best Logistic Regression Model (after GridSearchCV)')\n",
        "plt.show()\n",
        "\n",
        "fpr_best, tpr_best, thresholds_best = roc_curve(data_test_y, y_prob_best)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_best, tpr_best, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_best:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for Best Logistic Regression Model')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jAuA8_ZCZoay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ci mette 4 minuti"
      ],
      "metadata": {
        "id": "q8RyGwOWbIXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il tuning dei parametri non ha migliorato gli indici. Ha comportato un lieve peggioramento."
      ],
      "metadata": {
        "id": "QJH6_3eGb6HW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkVqNnixDP7e"
      },
      "source": [
        "## Randomized Search"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: utilizza RandomsizedSearchcV\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_dist = {\n",
        "    'logisticregression__C': np.logspace(-3, 2, 6),\n",
        "    'logisticregression__penalty': ['l2'],\n",
        "    'logisticregression__solver': ['lbfgs', 'liblinear'],\n",
        "    'logisticregression__max_iter': [1000, 2000, 3000, 5000],\n",
        "}\n",
        "\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=log_reg_OPT,\n",
        "                                   param_distributions=param_dist,\n",
        "                                   n_iter=48,\n",
        "                                   cv=3,\n",
        "                                   scoring='roc_auc',\n",
        "                                   n_jobs=-1,\n",
        "                                   random_state=42,\n",
        "                                   verbose=2)\n",
        "\n",
        "\n",
        "random_search.fit(data_train_x, data_train_y)\n",
        "\n",
        "\n",
        "print(\"Best parameters found by RandomizedSearchCV:\")\n",
        "print(random_search.best_params_)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HDRYzAKd6q10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = random_search.best_estimator_\n"
      ],
      "metadata": {
        "id": "RCHMQEsM9-hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Valuto la performance del nuovo modello"
      ],
      "metadata": {
        "id": "6b0YSLoQCOEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: valuta le performance di final model su test\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "# The code for evaluating the final_model on the test set is already present in the preceding code.\n",
        "# You just need to run the last few cells of the provided code block.\n",
        "\n",
        "# Evaluate the final_model (which is the best_estimator_ from RandomizedSearchCV) on the test set\n",
        "y_pred_final = final_model.predict(data_test_x)\n",
        "y_prob_final = final_model.predict_proba(data_test_x)[:, 1]\n",
        "\n",
        "# Calculate and print metrics for the final_model on the test set\n",
        "cm_final = confusion_matrix(data_test_y, y_pred_final)\n",
        "tn_final, fp_final, fn_final, tp_final = cm_final.ravel()\n",
        "\n",
        "accuracy_final = accuracy_score(data_test_y, y_pred_final)\n",
        "precision_final = precision_score(data_test_y, y_pred_final)\n",
        "sensitivity_final = recall_score(data_test_y, y_pred_final)\n",
        "specificity_final = tn_final / (tn_final + fp_final) if (tn_final + fp_final) > 0 else 0\n",
        "roc_auc_final = auc(roc_curve(data_test_y, y_prob_final)[0], roc_curve(data_test_y, y_prob_final)[1])\n",
        "\n",
        "print(\"\\nMetrics for the Final Model (from RandomizedSearchCV) on the Test Set:\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_final)\n",
        "print(f\"\\nAccuracy: {accuracy_final:.4f}\")\n",
        "print(f\"Precision: {precision_final:.4f}\")\n",
        "print(f\"Sensitivity (Recall): {sensitivity_final:.4f}\")\n",
        "print(f\"Specificity: {specificity_final:.4f}\")\n",
        "print(f\"Area Under the ROC Curve (AUC): {roc_auc_final:.4f}\")\n",
        "\n",
        "# Optional: Plot Confusion Matrix and ROC Curve for the final model\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_final, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "            xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion Matrix for Final Logistic Regression Model (after RandomizedSearchCV)')\n",
        "plt.show()\n",
        "\n",
        "fpr_final, tpr_final, thresholds_final = roc_curve(data_test_y, y_prob_final)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_final, tpr_final, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_final:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for Final Logistic Regression Model')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yE95SApa-RgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noto un peggioramento delle prestazioni"
      ],
      "metadata": {
        "id": "a90MfLZ4_DeN"
      }
    }
  ]
}