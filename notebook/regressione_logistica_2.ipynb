{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9mmFvwie1Pva0V0Y/py7q"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook per regressione lineare\n",
        "\n",
        "Di seguito:\n",
        "- verr√† implementato l'algoritmo di regressione logistica"
      ],
      "metadata": {
        "id": "CIs0Jhd77joY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaricamento dei dati"
      ],
      "metadata": {
        "id": "dMd2ihQ1ZpS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "class DataDownloaderExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom transformer to download and extract data from given URLs.\n",
        "\n",
        "    Args:\n",
        "        urls (list): A list of URLs to zip files.\n",
        "        output_dir (str): The directory to save the extracted files.\n",
        "    \"\"\"\n",
        "    def __init__(self, urls, output_dir=\"dataset\"):\n",
        "        self.urls = urls\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fits the transformer. In this case, it's a no-op as there's nothing to fit.\n",
        "\n",
        "        Args:\n",
        "            X: Input data (ignored).\n",
        "            y: Target data (ignored).\n",
        "\n",
        "        Returns:\n",
        "            self: The fitted transformer instance.\n",
        "        \"\"\"\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Downloads and extracts data from the provided URLs.\n",
        "\n",
        "        Args:\n",
        "            X: Input data (ignored).\n",
        "\n",
        "        Returns:\n",
        "            list: A list of paths to the extracted CSV files.\n",
        "        \"\"\"\n",
        "        for url in self.urls:\n",
        "            try:\n",
        "                print(f\"Downloading {url}...\")\n",
        "                response = requests.get(url, stream=True)\n",
        "                response.raise_for_status() # Raise an exception for bad status codes\n",
        "\n",
        "                # Read the zip file from the response content\n",
        "                with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
        "                    # Extract all contents to the specified output directory\n",
        "                    zip_ref.extractall(self.output_dir)\n",
        "                    print(f\"Extracted files from {url} to {self.output_dir}\")\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error downloading {url}: {e}\")\n",
        "            except zipfile.BadZipFile:\n",
        "                print(f\"Error: The downloaded file from {url} is not a valid zip file.\")\n",
        "            except Exception as e:\n",
        "                print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "        print(\"Download and extraction complete.\")\n",
        "        csv_files = glob.glob(os.path.join(self.output_dir, \"*.csv\"))\n",
        "        print(\"CSV files found:\", csv_files)\n",
        "        return csv_files\n",
        "\n",
        "# Example usage with a scikit-learn pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# List of URLs to your zipped files on AWS\n",
        "urls = [\n",
        "    \"https://phm-datasets.s3.amazonaws.com/Data_Challenge_PHM2024_training_data.zip\",\n",
        "]\n",
        "\n",
        "# Create the custom transformer\n",
        "downloader_extractor = DataDownloaderExtractor(urls=urls, output_dir=\"downloaded_data\")\n",
        "\n",
        "# You can add other steps to the pipeline if needed, e.g., a data loader\n",
        "# For this example, we just have the download and extract step\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('download_and_extract', downloader_extractor),\n",
        "    # Add more steps here if needed, e.g., loading the CSV files into pandas DataFrames\n",
        "])\n",
        "\n",
        "# Run the pipeline\n",
        "# The fit method is called first (though it does nothing in this transformer)\n",
        "# Then the transform method is called to perform the download and extraction\n",
        "extracted_files = pipeline.fit_transform(None) # Pass None as input data, as it's not used\n",
        "\n",
        "print(\"\\nPipeline execution complete.\")\n",
        "print(\"Extracted files:\", extracted_files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KabvZ7YaKIx",
        "outputId": "40652d06-56b4-4240-d1e1-c3d461a8865c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://phm-datasets.s3.amazonaws.com/Data_Challenge_PHM2024_training_data.zip...\n",
            "Extracted files from https://phm-datasets.s3.amazonaws.com/Data_Challenge_PHM2024_training_data.zip to downloaded_data\n",
            "Download and extraction complete.\n",
            "CSV files found: ['downloaded_data/X_train.csv', 'downloaded_data/y_train.csv']\n",
            "\n",
            "Pipeline execution complete.\n",
            "Extracted files: ['downloaded_data/X_train.csv', 'downloaded_data/y_train.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estrazione dei dati"
      ],
      "metadata": {
        "id": "er3mr_yCZyR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CSVLoader(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom transformer to load specific CSV files into pandas DataFrames\n",
        "    and return them in a format suitable for scikit-learn pipelines (e.g., as a tuple).\n",
        "\n",
        "    Assumes that the input X is a list of file paths, typically produced\n",
        "    by a previous step in the pipeline.\n",
        "\n",
        "    Args:\n",
        "        x_filename (str): The base name of the file containing features (e.g., 'X_train.csv').\n",
        "        y_filename (str): The base name of the file containing the target (e.g., 'y_train.csv').\n",
        "    \"\"\"\n",
        "    def __init__(self, x_filename='X_train.csv', y_filename='y_train.csv'):\n",
        "        self.x_filename = x_filename\n",
        "        self.y_filename = y_filename\n",
        "        self.x_data = None\n",
        "        self.y_data = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fits the transformer. This method will load the data.\n",
        "\n",
        "        Args:\n",
        "            X: A list of file paths (expected to contain x_filename and y_filename).\n",
        "            y: Target data (ignored).\n",
        "\n",
        "        Returns:\n",
        "            self: The fitted transformer instance.\n",
        "        \"\"\"\n",
        "        x_file_path = None\n",
        "        y_file_path = None\n",
        "\n",
        "        # Find the correct file paths in the input list\n",
        "        for file_path in X:\n",
        "            if os.path.basename(file_path) == self.x_filename:\n",
        "                x_file_path = file_path\n",
        "            elif os.path.basename(file_path) == self.y_filename:\n",
        "                y_file_path = file_path\n",
        "\n",
        "        if x_file_path is None:\n",
        "            raise FileNotFoundError(f\"Could not find {self.x_filename} in the provided file list.\")\n",
        "        if y_file_path is None:\n",
        "             raise FileNotFoundError(f\"Could not find {self.y_filename} in the provided file list.\")\n",
        "\n",
        "        try:\n",
        "            print(f\"Loading {x_file_path} into x_data...\")\n",
        "            self.x_data = pd.read_csv(x_file_path)\n",
        "            print(f\"Loading {y_file_path} into y_data...\")\n",
        "            self.y_data = pd.read_csv(y_file_path) # Or read_fwf depending on the format\n",
        "            print(\"Data loading complete.\")\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Error loading file: {e}\")\n",
        "            # You might want to re-raise the exception or handle it differently\n",
        "            raise\n",
        "        except pd.errors.EmptyDataError:\n",
        "            print(f\"Error: One of the files ({self.x_filename} or {self.y_filename}) is empty.\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred while loading data: {e}\")\n",
        "            raise\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Returns the loaded data (x_data, y_data).\n",
        "\n",
        "        Args:\n",
        "            X: Input data (ignored, data is loaded in fit).\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing (x_data, y_data) as pandas DataFrames.\n",
        "        \"\"\"\n",
        "        if self.x_data is None or self.y_data is None:\n",
        "             raise RuntimeError(\"Data has not been loaded yet. Call fit() first.\")\n",
        "\n",
        "        # Return the data in a format that can be passed to the next pipeline step\n",
        "        # For scikit-learn estimators, the fit method usually expects X and y separately.\n",
        "        # Returning a tuple (X, y) allows the next step's fit method to receive them.\n",
        "        return (self.x_data, self.y_data)\n",
        "\n",
        "# Add the CSVLoader to the pipeline\n",
        "pipeline_with_loading = Pipeline([\n",
        "    ('download_and_extract', downloader_extractor),\n",
        "    ('load_csv', CSVLoader(x_filename='X_train.csv', y_filename='y_train.csv'))\n",
        "    # Add more steps here, e.g., preprocessing, model training\n",
        "])\n",
        "\n",
        "# Run the pipeline\n",
        "# The output of the 'load_csv' step will be a tuple (x_data, y_data)\n",
        "loaded_data = pipeline_with_loading.fit_transform(None)\n",
        "\n",
        "# Access the loaded data\n",
        "x_data, y_data = loaded_data\n",
        "\n",
        "print(\"\\nLoaded x_data shape:\", x_data.shape)\n",
        "print(\"Loaded y_data shape:\", y_data.shape)\n",
        "print(\"\\nFirst 5 rows of x_data:\")\n",
        "print(x_data.head())\n",
        "print(\"\\nFirst 5 rows of y_data:\")\n",
        "print(y_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVB_ziOyaymE",
        "outputId": "b6e4d175-b2b9-4a1c-a557-b00cd38122d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://phm-datasets.s3.amazonaws.com/Data_Challenge_PHM2024_training_data.zip...\n",
            "Extracted files from https://phm-datasets.s3.amazonaws.com/Data_Challenge_PHM2024_training_data.zip to downloaded_data\n",
            "Download and extraction complete.\n",
            "CSV files found: ['downloaded_data/X_train.csv', 'downloaded_data/y_train.csv']\n",
            "Loading downloaded_data/X_train.csv into x_data...\n",
            "Loading downloaded_data/y_train.csv into y_data...\n",
            "Data loading complete.\n",
            "\n",
            "Loaded x_data shape: (742625, 8)\n",
            "Loaded y_data shape: (742625, 3)\n",
            "\n",
            "First 5 rows of x_data:\n",
            "   id  trq_measured       oat       mgt         pa       ias         np  \\\n",
            "0   0        54.100   2.00000  544.5000   212.1408  74.56250   89.18000   \n",
            "1   1        49.625  24.22231  578.4844  1625.6400  30.35596   99.55273   \n",
            "2   2        52.000   7.00000  566.1000  1912.9250  65.62500  100.14000   \n",
            "3   3        62.400   7.25000  560.1000   277.0632  54.81250   90.64000   \n",
            "4   4        62.900  23.25000  593.7000    53.6448  73.43750   99.91000   \n",
            "\n",
            "         ng  \n",
            "0   99.6400  \n",
            "1   91.3866  \n",
            "2   90.9600  \n",
            "3  100.2800  \n",
            "4   92.1700  \n",
            "\n",
            "First 5 rows of y_data:\n",
            "   id  faulty  trq_margin\n",
            "0   0       1  -13.717745\n",
            "1   1       0    1.791863\n",
            "2   2       1  -13.944871\n",
            "3   3       0   -0.017281\n",
            "4   4       0    7.322404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creazione train-set e test-set"
      ],
      "metadata": {
        "id": "7DrzNQ6kbh4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class DataPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom transformer to preprocess the data:\n",
        "    - Drop the 'id' column from x_data.\n",
        "    - Merge the 'faulty' column from y_data into x_data.\n",
        "    - Split the merged data into training and testing sets.\n",
        "\n",
        "    Assumes the input is a tuple (x_data, y_data) as pandas DataFrames,\n",
        "    typically from a previous pipeline step like CSVLoader.\n",
        "    \"\"\"\n",
        "    def __init__(self, test_size=0.2, random_state=None):\n",
        "        self.test_size = test_size\n",
        "        self.random_state = random_state\n",
        "        self.data_train = None\n",
        "        self.data_test = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fits the transformer by preprocessing and splitting the data.\n",
        "\n",
        "        Args:\n",
        "            X: A tuple (x_data, y_data) where x_data is the features DataFrame\n",
        "               and y_data is the target DataFrame.\n",
        "            y: Target data (ignored, as the target is expected in y_data).\n",
        "\n",
        "        Returns:\n",
        "            self: The fitted transformer instance.\n",
        "        \"\"\"\n",
        "        if not isinstance(X, tuple) or len(X) != 2:\n",
        "            raise TypeError(\"Input X must be a tuple (x_data, y_data).\")\n",
        "\n",
        "        x_data, y_data = X\n",
        "\n",
        "        if not isinstance(x_data, pd.DataFrame) or not isinstance(y_data, pd.DataFrame):\n",
        "             raise TypeError(\"Both elements in the input tuple must be pandas DataFrames.\")\n",
        "\n",
        "        # Drop the 'id' column from x_data if it exists\n",
        "        if 'id' in x_data.columns:\n",
        "            print(\"Dropping 'id' column from x_data...\")\n",
        "            x_data_processed = x_data.drop('id', axis=1)\n",
        "        else:\n",
        "            print(\"'id' column not found in x_data. Skipping drop.\")\n",
        "            x_data_processed = x_data.copy()\n",
        "\n",
        "        # Check if 'faulty' column exists in y_data and merge it\n",
        "        if 'faulty' in y_data.columns:\n",
        "            print(\"Merging 'faulty' column from y_data into x_data...\")\n",
        "            # Ensure dataframes can be merged, e.g., they have a common index or column\n",
        "            # Assuming they can be concatenated side-by-side based on index\n",
        "            # If merging by a specific column is needed, adjust here\n",
        "            merged_data = pd.concat([x_data_processed, y_data['faulty']], axis=1)\n",
        "        else:\n",
        "             raise ValueError(\"'faulty' column not found in y_data.\")\n",
        "\n",
        "        print(f\"Splitting data into train ({1-self.test_size:.0%}) and test ({self.test_size:.0%})...\")\n",
        "        # Split the merged data into training and testing sets\n",
        "        self.data_train, self.data_test = train_test_split(\n",
        "            merged_data,\n",
        "            test_size=self.test_size,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "        print(\"Data splitting complete.\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Returns the split training and testing data.\n",
        "\n",
        "        Args:\n",
        "            X: Input data (ignored, splitting is done in fit).\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing (data_train, data_test) as pandas DataFrames.\n",
        "        \"\"\"\n",
        "        if self.data_train is None or self.data_test is None:\n",
        "             raise RuntimeError(\"Data has not been preprocessed or split yet. Call fit() first.\")\n",
        "\n",
        "        # Return the split data\n",
        "        return (self.data_train, self.data_test)\n",
        "\n",
        "# Extend the existing pipeline to include the DataPreprocessor\n",
        "pipeline_with_preprocessing = Pipeline([\n",
        "    ('download_and_extract', downloader_extractor),\n",
        "    ('load_csv', CSVLoader(x_filename='X_train.csv', y_filename='y_train.csv')),\n",
        "    ('preprocess_and_split', DataPreprocessor(test_size=0.2, random_state=42)) # Add the preprocessor\n",
        "    # Add more steps here, e.g., feature scaling, model training\n",
        "])\n",
        "\n",
        "# Run the pipeline\n",
        "# The output of the 'preprocess_and_split' step will be a tuple (data_train, data_test)\n",
        "split_data = pipeline_with_preprocessing.fit_transform(None)\n",
        "\n",
        "# Access the split data\n",
        "data_train, data_test = split_data\n",
        "\n",
        "print(\"\\nProcessed and Split Data:\")\n",
        "print(\"data_train shape:\", data_train.shape)\n",
        "print(\"data_test shape:\", data_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cjdq-1HpbnNY",
        "outputId": "ded7fb40-b343-4d6a-e4d4-83bc19f7ff5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://phm-datasets.s3.amazonaws.com/Data_Challenge_PHM2024_training_data.zip...\n",
            "Extracted files from https://phm-datasets.s3.amazonaws.com/Data_Challenge_PHM2024_training_data.zip to downloaded_data\n",
            "Download and extraction complete.\n",
            "CSV files found: ['downloaded_data/X_train.csv', 'downloaded_data/y_train.csv']\n",
            "Loading downloaded_data/X_train.csv into x_data...\n",
            "Loading downloaded_data/y_train.csv into y_data...\n",
            "Data loading complete.\n",
            "Dropping 'id' column from x_data...\n",
            "Merging 'faulty' column from y_data into x_data...\n",
            "Splitting data into train (80%) and test (20%)...\n",
            "Data splitting complete.\n",
            "\n",
            "Processed and Split Data:\n",
            "data_train shape: (594100, 8)\n",
            "data_test shape: (148525, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful package\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def transform_with_custom_root(df, column_name, root_degree):\n",
        "  \"\"\"\n",
        "  Applies a custom root transformation (1/root_degree power) to a column.\n",
        "  Handles positive, negative, and zero values appropriately based on the root degree.\n",
        "\n",
        "  Args:\n",
        "    df (pd.DataFrame): The input DataFrame.\n",
        "    column_name (str): The name of the column to transform.\n",
        "    root_degree (float): The degree of the root (e.g., 2 for square root, 3 for cube root).\n",
        "\n",
        "  Returns:\n",
        "    pd.DataFrame: The DataFrame with the transformed column.\n",
        "  \"\"\"\n",
        "  new_column_name = f'{column_name}_root_{root_degree:.2f}_transformed'\n",
        "\n",
        "  if root_degree == 0:\n",
        "      raise ValueError(\"Root degree cannot be zero.\")\n",
        "  elif root_degree % 2 == 0:  # Even root\n",
        "      # For even roots, we can only take the root of non-negative numbers\n",
        "      if (df[column_name] < 0).any():\n",
        "          print(f\"Warning: Column '{column_name}' contains negative values. Cannot apply even root directly.\")\n",
        "          # You might choose to handle this by taking the root of the absolute value,\n",
        "          # or setting negative values to NaN, depending on your data context.\n",
        "          # Here, we'll take the root of the absolute value for demonstration.\n",
        "          df[new_column_name] = np.power(np.abs(df[column_name]), 1/root_degree)\n",
        "      else:\n",
        "          df[new_column_name] = np.power(df[column_name], 1/root_degree)\n",
        "  else:  # Odd root\n",
        "      # Odd roots can handle positive, negative, and zero values\n",
        "      df[new_column_name] = np.sign(df[column_name]) * np.power(np.abs(df[column_name]), 1/root_degree)\n",
        "\n",
        "  return df\n",
        "\n",
        "# Example usage with a custom root (e.g., 1.5)\n",
        "# custom_root_degree = 2.35\n",
        "# data_train = transform_with_custom_root(data_train.copy(), 'power_avail', custom_root_degree)\n",
        "\n",
        "\n",
        "def create_binned_qualitative_variable(df, column_name, num_bins, strategy='quantile'):\n",
        "  \"\"\"\n",
        "  Creates a qualitative (categorical) variable by binning a numerical column.\n",
        "\n",
        "  Args:\n",
        "    df (pd.DataFrame): The input DataFrame.\n",
        "    column_name (str): The name of the numerical column to bin.\n",
        "    num_bins (int): The desired number of bins.\n",
        "    strategy (str): The strategy to use for binning. 'quantile' uses quantiles\n",
        "                    to ensure bins have approximately equal numbers of observations.\n",
        "                    'uniform' creates bins with equal widths. Default is 'quantile'.\n",
        "\n",
        "  Returns:\n",
        "    pd.DataFrame: The DataFrame with a new qualitative column.\n",
        "                  The new column name will be f'{column_name}_binned_{num_bins}_{strategy}'.\n",
        "  \"\"\"\n",
        "  if column_name not in df.columns:\n",
        "    raise ValueError(f\"La colonna '{column_name}' non √® presente nel DataFrame.\")\n",
        "  if num_bins <= 1:\n",
        "      raise ValueError(\"Il numero di bins deve essere maggiore di 1.\")\n",
        "\n",
        "  new_column_name = f'{column_name}_binned_{num_bins}_{strategy}'\n",
        "\n",
        "  if strategy == 'quantile':\n",
        "    # Use qcut to create bins based on quantiles (approximately equal number of observations)\n",
        "    # `duplicates='drop'` handles cases where quantile boundaries are not unique,\n",
        "    # which can happen with skewed or discrete data.\n",
        "    df[new_column_name] = pd.qcut(df[column_name], q=num_bins, labels=False, duplicates='drop')\n",
        "  elif strategy == 'uniform':\n",
        "    # Use cut to create bins of equal width\n",
        "    df[new_column_name] = pd.cut(df[column_name], bins=num_bins, labels=False, include_lowest=True)\n",
        "  else:\n",
        "    raise ValueError(f\"Strategia di binning non valida: '{strategy}'. Scegliere tra 'quantile' o 'uniform'.\")\n",
        "\n",
        "  # Convert the binned column to object/category type if needed, or keep as int for simplicity\n",
        "  # Here we keep it as int representing the bin number\n",
        "\n",
        "  return df\n",
        "\n",
        "# Example usage for 'indicated_air_speed':\n",
        "# num_bins_indicated_air_speed = 5 # Define the number of bins\n",
        "# binning_strategy = 'quantile' # Or 'uniform'\n",
        "\n",
        "#data_train = create_binned_qualitative_variable(\n",
        "#    data_train.copy(),\n",
        "#    'indicated_air_speed',\n",
        "#    num_bins_indicated_air_speed,\n",
        "#    strategy=binning_strategy\n",
        "#)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## PCA per indicated_air_speed e compressor_speed\n",
        "# Select the columns for PCA\n",
        "# features_for_pca = data_train[['compressor_speed', 'net_power']]\n",
        "# Initialize PCA with 1 component (to combine the two variables)\n",
        "# pca = PCA(n_components=1)\n",
        "# Fit PCA on the selected features and transform them\n",
        "# data_train['compressor_speed_net_power_pca'] = pca.fit_transform(features_for_pca)\n",
        "\n",
        "\n",
        "\n",
        "## Creazione di torque_times_temp\n",
        "\n",
        "# data_train['torque_times_temp'] = data_train['torque_meas'] * data_train['outside_air_temp']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Creazione pipeline\n",
        "def prepare_data_pipeline(x_path, y_path, new_column_names=None,\n",
        "                          root_transformations=None,\n",
        "                          binning_config=None,\n",
        "                          standardize=True,\n",
        "                          drop_index_col='idx'):\n",
        "    \"\"\"\n",
        "    Esegue la pipeline completa di preprocessing.\n",
        "\n",
        "    Args:\n",
        "        x_path (str): path al file X_train.csv\n",
        "        y_path (str): path al file y_train.csv\n",
        "        new_column_names (list): lista di nuovi nomi colonne (opzionale)\n",
        "        root_transformations (dict): dict {colonna: radice}\n",
        "        binning_config (dict): dict {colonna: (num_bins, strategia)}\n",
        "        standardize (bool): se standardizzare le colonne numeriche\n",
        "        drop_index_col (str): nome della colonna da droppare (opzionale)\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame preprocessato pronto per il training\n",
        "    \"\"\"\n",
        "    df = load_training_data(x_path, y_path)\n",
        "\n",
        "    if new_column_names:\n",
        "        df = rename_dataframe_columns(df, new_column_names + ['y_target'])\n",
        "\n",
        "    if drop_index_col in df.columns:\n",
        "        df = df.drop(drop_index_col, axis=1)\n",
        "\n",
        "    # Trasformazioni custom root\n",
        "    if root_transformations:\n",
        "        for col, deg in root_transformations.items():\n",
        "            df = transform_with_custom_root(df, col, deg)\n",
        "\n",
        "    # Binning\n",
        "    if binning_config:\n",
        "        for col, (n_bins, strategy) in binning_config.items():\n",
        "            df = create_binned_qualitative_variable(df, col, n_bins, strategy)\n",
        "\n",
        "    # PCA: esempio hardcoded ma puoi parametrizzare se vuoi\n",
        "    if {'compressor_speed', 'net_power'}.issubset(df.columns):\n",
        "        pca = PCA(n_components=1)\n",
        "        df['compressor_speed_net_power_pca'] = pca.fit_transform(df[['compressor_speed', 'net_power']])\n",
        "\n",
        "    # Feature engineering manuale\n",
        "    if {'torque_meas', 'outside_air_temp'}.issubset(df.columns):\n",
        "        df['torque_times_temp'] = df['torque_meas'] * df['outside_air_temp']\n",
        "\n",
        "    # Rimuovi colonne non necessarie\n",
        "    columns_to_drop = ['compressor_speed','net_power','indicated_air_speed','power_avail']  # Aggiungi qui altre colonne da rimuovere\n",
        "    df = df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "    # Assicurati che 'y_target' sia l'ultima colonna\n",
        "    if 'y_target' in df.columns:\n",
        "        cols = [col for col in df.columns if col != 'y_target'] + ['y_target']\n",
        "        df = df[cols]\n",
        "    else:\n",
        "        print(\"Warning: 'y_target' column not found in DataFrame. It will not be moved to the end.\")\n",
        "\n",
        "    # Assicurati che il DataFrame non abbia colonne duplicate\n",
        "    df = df.loc[:, ~df.columns.duplicated()]\n",
        "\n",
        "    # Assicurati che il DataFrame non abbia valori NaN\n",
        "    if df.isnull().values.any():\n",
        "        print(\"Warning: DataFrame contains NaN values. They will be filled with 0.\")\n",
        "        df = df.fillna(0)\n",
        "\n",
        "    # Assicurati che il DataFrame non abbia valori infiniti\n",
        "    if np.isinf(df.values).any():\n",
        "        print(\"Warning: DataFrame contains infinite values. They will be replaced with 0.\")\n",
        "        df.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "\n",
        "    # Standardizzazione\n",
        "    if standardize:\n",
        "        target = 'y_target'\n",
        "        numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "        columns_to_standardize = [col for col in numerical_cols if col != target]\n",
        "        df = standardize_columns(df, columns_to_standardize)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "### Esempio di utilizzo della pipeline\n",
        "x_path = 'dataset/X_data.csv'\n",
        "y_path = 'dataset/y_data.csv'\n",
        "\n",
        "# Configurazioni opzionali\n",
        "new_column_names = ['idx', 'torque_meas', 'outside_air_temp', 'mean_gas_temp',\n",
        "                    'power_avail', 'indicated_air_speed', 'net_power', 'compressor_speed']\n",
        "\n",
        "root_transform = {'power_avail': 2.35}\n",
        "binning = {'indicated_air_speed': (5, 'quantile')}\n",
        "\n",
        "data_ready = prepare_data_pipeline(\n",
        "    x_path, y_path,\n",
        "    new_column_names=new_column_names,\n",
        "    root_transformations=root_transform,\n",
        "    binning_config=binning\n",
        ")\n",
        "\n",
        "# Esempio di stampa del DataFrame preprocessato\n",
        "print(data_ready.head())\n",
        "print(data_ready.describe())\n",
        "\n",
        "n_cols = 3\n",
        "n_rows = (len(data_ready.columns) + n_cols - 1) // n_cols\n",
        "plt.figure(figsize=(15, n_rows * 4))\n",
        "for i, col in enumerate(data_ready):\n",
        "    plt.subplot(n_rows, n_cols, i + 1)\n",
        "    sns.histplot(data_ready[col], bins=50, kde=True)\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"grafico.png\")\n",
        "\n",
        "# data_ready ora √® pronto per essere usato in un modello"
      ],
      "metadata": {
        "id": "SYD6QZ_spo-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Radice custom"
      ],
      "metadata": {
        "id": "ZSURPvKHuL8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trasformazione"
      ],
      "metadata": {
        "id": "xnmnqu9lwe61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crea una classe che sia compatibile con l'oggetto pipeline di scikit che contenga la funzione transform_with_custom_root\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "class CustomRootTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom transformer to apply a custom root transformation (1/root_degree power)\n",
        "    to specified columns.\n",
        "\n",
        "    Handles positive, negative, and zero values appropriately based on the root degree.\n",
        "\n",
        "    Args:\n",
        "        root_transformations (dict): A dictionary where keys are column names\n",
        "                                     and values are the root degrees to apply.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_transformations=None):\n",
        "        self.root_transformations = root_transformations if root_transformations is not None else {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fits the transformer. This transformer does not need to be fitted,\n",
        "        it just checks if the columns exist.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): The input DataFrame.\n",
        "            y: Target data (ignored).\n",
        "\n",
        "        Returns:\n",
        "            self: The fitted transformer instance.\n",
        "        \"\"\"\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
        "\n",
        "        for col in self.root_transformations.keys():\n",
        "            if col not in X.columns:\n",
        "                raise ValueError(f\"Column '{col}' specified for root transformation not found in the input DataFrame.\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Applies the custom root transformation to the specified columns.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): The input DataFrame.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: The DataFrame with the transformed columns.\n",
        "        \"\"\"\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
        "\n",
        "        # Create a copy to avoid modifying the original DataFrame\n",
        "        df_transformed = X.copy()\n",
        "\n",
        "        for column_name, root_degree in self.root_transformations.items():\n",
        "            if column_name in df_transformed.columns:\n",
        "                new_column_name = f'{column_name}_root_{root_degree:.2f}_transformed'\n",
        "\n",
        "                if root_degree == 0:\n",
        "                    print(f\"Warning: Root degree for column '{column_name}' is zero. Skipping transformation.\")\n",
        "                    continue\n",
        "                elif root_degree % 2 == 0:  # Even root\n",
        "                    # For even roots, take the root of the absolute value and preserve sign\n",
        "                    if (df_transformed[column_name] < 0).any():\n",
        "                         print(f\"Applying even root to column '{column_name}' which contains negative values. Using absolute value.\")\n",
        "                    df_transformed[new_column_name] = np.power(np.abs(df_transformed[column_name]), 1/root_degree)\n",
        "                    # Note: For even roots of negative numbers, the result is imaginary.\n",
        "                    # Taking the root of absolute value and preserving sign is one way to handle this,\n",
        "                    # but might not be mathematically appropriate depending on the context.\n",
        "                    # If imaginary numbers are expected, use cmath and handle complex outputs.\n",
        "                    # For simplicity here, we take abs root.\n",
        "\n",
        "                else:  # Odd root\n",
        "                    # Odd roots can handle positive, negative, and zero values\n",
        "                    df_transformed[new_column_name] = np.sign(df_transformed[column_name]) * np.power(np.abs(df_transformed[column_name]), 1/root_degree)\n",
        "            else:\n",
        "                print(f\"Warning: Column '{column_name}' not found in DataFrame. Skipping root transformation for this column.\")\n",
        "\n",
        "        return df_transformed\n"
      ],
      "metadata": {
        "id": "hDJsTcYkqUBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Esempio di utilizzo"
      ],
      "metadata": {
        "id": "zhAxtn-5ucJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage with a pipeline\n",
        "# Extend the existing pipeline to include the CustomRootTransformer\n",
        "\n",
        "# Define your root transformations\n",
        "root_transform = {'power_avail': 2.35, 'net_power': 3} # Example: apply cube root to 'net_power'\n",
        "\n",
        "pipeline_with_root_transform = Pipeline([\n",
        "    ('download_and_extract', DataDownloaderExtractor(urls=urls, output_dir=\"downloaded_data\")),\n",
        "    ('load_csv', CSVLoader(x_filename='X_train.csv', y_filename='y_train.csv')),\n",
        "    # You might need a step here to merge x_data and y_data or select relevant columns\n",
        "    # before applying transformations like root, depending on where these columns are.\n",
        "    # Assuming for this example, we get a DataFrame where 'power_avail' and 'net_power' exist.\n",
        "    # A simple step to create a combined DataFrame:\n",
        "    ('combine_data', DataPreprocessor(test_size=0.0, random_state=None)), # Use test_size=0.0 to keep all data combined\n",
        "    ('apply_root_transform', CustomRootTransformer(root_transformations=root_transform))\n",
        "    # Add other steps like binning, PCA, standardization after this if needed\n",
        "])\n",
        "\n",
        "# Run the pipeline up to the root transformation step\n",
        "# The output of the 'combine_data' step will be the merged DataFrame (data_train)\n",
        "transformed_data = pipeline_with_root_transform.fit_transform(None)\n",
        "\n",
        "print(\"\\nData after root transformation:\")\n",
        "print(transformed_data.head())\n",
        "\n",
        "# Check if the new columns were added\n",
        "print(\"\\nColumns after root transformation:\")\n",
        "transformed_data.columns"
      ],
      "metadata": {
        "id": "GR3NqC2iuRDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantile_binary"
      ],
      "metadata": {
        "id": "CKNFnVhcuRiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trasformazione"
      ],
      "metadata": {
        "id": "teNr1y8MwUHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crea una classe che sia compatibile con l'oggetto pipeline di scikit che contenga la funzione transform_with_quantile_transformer\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "class QuantileTransformerWrapper(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom transformer that applies a QuantileTransformer to a specified column.\n",
        "\n",
        "    Args:\n",
        "        column_name (str): The name of the column to transform.\n",
        "        output_distribution (str): The desired output distribution. 'normal'\n",
        "                                   for Gaussian-like, 'uniform' for uniform.\n",
        "                                   Default is 'normal'.\n",
        "        n_quantiles (int or None): Number of quantiles to be computed. If None,\n",
        "                                   it is set to min(n_samples, 1000).\n",
        "    \"\"\"\n",
        "    def __init__(self, column_name, output_distribution='normal', n_quantiles=None):\n",
        "        self.column_name = column_name\n",
        "        self.output_distribution = output_distribution\n",
        "        self.n_quantiles = n_quantiles\n",
        "        self.transformer = None\n",
        "        self.new_column_name = f'{self.column_name}_quantile_transformed'\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fits the QuantileTransformer on the specified column.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): The input DataFrame.\n",
        "            y: Target data (ignored).\n",
        "\n",
        "        Returns:\n",
        "            self: The fitted transformer instance.\n",
        "        \"\"\"\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            raise TypeError(\"Input must be a pandas DataFrame.\")\n",
        "        if self.column_name not in X.columns:\n",
        "            raise ValueError(f\"Column '{self.column_name}' not found in the DataFrame.\")\n",
        "\n",
        "        data_to_fit = X[self.column_name].values.reshape(-1, 1)\n",
        "\n",
        "        self.transformer = QuantileTransformer(\n",
        "            output_distribution=self.output_distribution,\n",
        "            n_quantiles=self.n_quantiles\n",
        "        )\n",
        "        self.transformer.fit(data_to_fit)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Applies the fitted QuantileTransformer to the specified column.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): The input DataFrame.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: The DataFrame with the transformed column added.\n",
        "        \"\"\"\n",
        "        if self.transformer is None:\n",
        "            raise RuntimeError(\"Transformer has not been fitted yet. Call fit() first.\")\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            raise TypeError(\"Input must be a pandas DataFrame.\")\n",
        "        if self.column_name not in X.columns:\n",
        "            raise ValueError(f\"Column '{self.column_name}' not found in the DataFrame.\")\n",
        "\n",
        "        df_transformed = X.copy()\n",
        "\n",
        "        data_to_transform = df_transformed[self.column_name].values.reshape(-1, 1)\n",
        "        transformed_data = self.transformer.transform(data_to_transform)\n",
        "\n",
        "        df_transformed[self.new_column_name] = transformed_data\n",
        "\n",
        "        return df_transformed\n",
        "\n"
      ],
      "metadata": {
        "id": "x0k8KZkLt3MC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Esempio di utilizzo"
      ],
      "metadata": {
        "id": "cGa8PjE-wJOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage within a pipeline:\n",
        "# Add this transformer to the existing pipeline after loading and preprocessing\n",
        "\n",
        "# Add the QuantileTransformerWrapper to the pipeline\n",
        "pipeline_with_quantile = Pipeline([\n",
        "    ('download_and_extract', downloader_extractor),\n",
        "    ('load_csv', CSVLoader(x_filename='X_train.csv', y_filename='y_train.csv')),\n",
        "    ('preprocess_and_split', DataPreprocessor(test_size=0.2, random_state=42)),\n",
        "    # Access the first element of the tuple (data_train) for transformation\n",
        "    # Note: This is a simplified example. In a real pipeline, you might\n",
        "    # need a custom step to handle the tuple output from preprocess_and_split\n",
        "    # or apply transformations before the split.\n",
        "    # For demonstration, let's assume we are applying it to the entire loaded x_data\n",
        "    # *before* splitting, or we apply it only to data_train after splitting.\n",
        "    # A better way is to apply transformations to the full dataset or use ColumnTransformer\n",
        "    # after the split if different transformations are needed for different columns.\n",
        "\n",
        "    # Let's modify the pipeline to apply transformations *before* the split for simplicity.\n",
        "    # We'll need a new step to combine the data before splitting.\n",
        "\n",
        "])\n",
        "\n",
        "# Re-create the pipeline structure to allow transformations on the full data before splitting\n",
        "# or a more complex structure to handle the tuple output.\n",
        "\n",
        "# Option 1: Transform full data before splitting (requires modifying DataPreprocessor or adding a combiner)\n",
        "# This is often cleaner if transformations apply to both train/test sets.\n",
        "\n",
        "# Option 2: Apply transformations *after* splitting. This needs careful handling of the tuple output.\n",
        "# A common approach is to apply a ColumnTransformer *within* a branch of the pipeline,\n",
        "# or apply transformations to the train/test sets separately after the split.\n",
        "\n",
        "# Let's create a simplified example applying the transformer to the loaded x_data\n",
        "# before the split for demonstration. We need a step to combine X and y back.\n",
        "# This would require modifying the CSVLoader or adding a new step.\n",
        "\n",
        "# Let's instead show how to apply the QuantileTransformerWrapper *after* getting the split data,\n",
        "# outside the existing pipeline, for clarity.\n",
        "\n",
        "# Run the pipeline up to the split step\n",
        "split_data = pipeline_with_preprocessing.fit_transform(None)\n",
        "data_train, data_test = split_data\n",
        "\n",
        "# Now apply the QuantileTransformerWrapper to a column in the training data\n",
        "# and the testing data separately (which is standard practice in ML workflows).\n",
        "\n",
        "# Example: Apply quantile transformation to 'net_power' in data_train\n",
        "quantile_transformer_net_power_train = QuantileTransformerWrapper(\n",
        "    column_name='net_power',\n",
        "    output_distribution='normal'\n",
        ")\n",
        "\n",
        "# Fit and transform on the training data\n",
        "data_train_transformed = quantile_transformer_net_power_train.fit_transform(data_train.copy())\n",
        "\n",
        "# Only transform on the test data using the *same* fitted transformer\n",
        "data_test_transformed = quantile_transformer_net_power_train.transform(data_test.copy())\n",
        "\n",
        "print(\"\\nDataTrain after Quantile Transformation:\")\n",
        "print(data_train_transformed[[\n",
        "    'net_power',\n",
        "    'net_power_quantile_transformed',\n",
        "    'y_target' # Include target to see context\n",
        "]].head())\n",
        "\n",
        "print(\"\\nDataTest after Quantile Transformation:\")\n",
        "print(data_test_transformed[[\n",
        "    'net_power',\n",
        "    'net_power_quantile_transformed',\n",
        "    'y_target' # Include target to see context\n",
        "]].head())\n",
        "\n",
        "# To integrate this into a pipeline that handles the split data tuple:\n",
        "# You would typically define a step that expects the tuple (data_train, data_test)\n",
        "# and applies transformers to specific columns within those DataFrames.\n",
        "# This often involves using scikit-learn's ColumnTransformer inside a custom step\n",
        "# or applying transformers conditionally based on whether it's training or testing.\n",
        "\n",
        "# A more sophisticated pipeline structure might look like:\n",
        "# Pipeline([\n",
        "#     ('download_and_extract', DataDownloaderExtractor(...)),\n",
        "#     ('load_csv', CSVLoader(...)),\n",
        "#     ('combine_xy', CustomXYCombiner()), # Custom step to combine X and y back into one df\n",
        "#     ('split_data', CustomTrainTestSplitter(...)), # Custom step to return train/test tuple\n",
        "#     ('transform_data', CustomDataTransformerStep(...)), # Custom step that takes (train, test) tuple\n",
        "#         # Inside CustomDataTransformerStep, you would apply things like QuantileTransformerWrapper,\n",
        "#         # CustomRootTransformer, etc., using ColumnTransformer maybe.\n",
        "#     ('model', YourModel())\n",
        "# ])\n",
        "\n",
        "# For this request, the class `QuantileTransformerWrapper` is provided,\n",
        "# which fulfills the requirement of being compatible with the scikit-learn pipeline object.\n",
        "# Its `fit` and `transform` methods are defined according to the TransformerMixin interface.\n",
        "# You would integrate it into a suitable pipeline structure depending on your workflow\n",
        "# (e.g., applied before or after the train/test split, using ColumnTransformer).\n"
      ],
      "metadata": {
        "id": "tyVa8PhMtnhj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}