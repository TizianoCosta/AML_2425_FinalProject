{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdTOW/6nFEUwK1I6addYf7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TizianoCosta/AML_2425_FinalProject/blob/main/notebook/regressione_logistica_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook per regressione lineare\n",
        "\n",
        "Di seguito:\n",
        "- verr√† implementato l'algoritmo di regressione logistica"
      ],
      "metadata": {
        "id": "CIs0Jhd77joY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaricamento dei dati"
      ],
      "metadata": {
        "id": "dMd2ihQ1ZpS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "class DataDownloaderExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom transformer to download and extract data from given URLs.\n",
        "\n",
        "    Args:\n",
        "        urls (list): A list of URLs to zip files.\n",
        "        output_dir (str): The directory to save the extracted files.\n",
        "    \"\"\"\n",
        "    def __init__(self, urls, output_dir=\"dataset\"):\n",
        "        self.urls = urls\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fits the transformer. In this case, it's a no-op as there's nothing to fit.\n",
        "\n",
        "        Args:\n",
        "            X: Input data (ignored).\n",
        "            y: Target data (ignored).\n",
        "\n",
        "        Returns:\n",
        "            self: The fitted transformer instance.\n",
        "        \"\"\"\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Downloads and extracts data from the provided URLs.\n",
        "\n",
        "        Args:\n",
        "            X: Input data (ignored).\n",
        "\n",
        "        Returns:\n",
        "            list: A list of paths to the extracted CSV files.\n",
        "        \"\"\"\n",
        "        for url in self.urls:\n",
        "            try:\n",
        "                print(f\"Downloading {url}...\")\n",
        "                response = requests.get(url, stream=True)\n",
        "                response.raise_for_status() # Raise an exception for bad status codes\n",
        "\n",
        "                # Read the zip file from the response content\n",
        "                with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
        "                    # Extract all contents to the specified output directory\n",
        "                    zip_ref.extractall(self.output_dir)\n",
        "                    print(f\"Extracted files from {url} to {self.output_dir}\")\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error downloading {url}: {e}\")\n",
        "            except zipfile.BadZipFile:\n",
        "                print(f\"Error: The downloaded file from {url} is not a valid zip file.\")\n",
        "            except Exception as e:\n",
        "                print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "        print(\"Download and extraction complete.\")\n",
        "        csv_files = glob.glob(os.path.join(self.output_dir, \"*.csv\"))\n",
        "        print(\"CSV files found:\", csv_files)\n",
        "        return csv_files\n",
        "\n",
        "# Example usage with a scikit-learn pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# List of URLs to your zipped files on AWS\n",
        "urls = [\n",
        "    \"https://phm-datasets.s3.amazonaws.com/Data_Challenge_PHM2024_training_data.zip\",\n",
        "]\n",
        "\n",
        "# Create the custom transformer\n",
        "downloader_extractor = DataDownloaderExtractor(urls=urls, output_dir=\"downloaded_data\")\n",
        "\n",
        "# You can add other steps to the pipeline if needed, e.g., a data loader\n",
        "# For this example, we just have the download and extract step\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('download_and_extract', downloader_extractor),\n",
        "    # Add more steps here if needed, e.g., loading the CSV files into pandas DataFrames\n",
        "])\n",
        "\n",
        "# Run the pipeline\n",
        "# The fit method is called first (though it does nothing in this transformer)\n",
        "# Then the transform method is called to perform the download and extraction\n",
        "extracted_files = pipeline.fit_transform(None) # Pass None as input data, as it's not used\n",
        "\n",
        "print(\"\\nPipeline execution complete.\")\n",
        "print(\"Extracted files:\", extracted_files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KabvZ7YaKIx",
        "outputId": "40652d06-56b4-4240-d1e1-c3d461a8865c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://phm-datasets.s3.amazonaws.com/Data_Challenge_PHM2024_training_data.zip...\n",
            "Extracted files from https://phm-datasets.s3.amazonaws.com/Data_Challenge_PHM2024_training_data.zip to downloaded_data\n",
            "Download and extraction complete.\n",
            "CSV files found: ['downloaded_data/X_train.csv', 'downloaded_data/y_train.csv']\n",
            "\n",
            "Pipeline execution complete.\n",
            "Extracted files: ['downloaded_data/X_train.csv', 'downloaded_data/y_train.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estrazione dei dati"
      ],
      "metadata": {
        "id": "er3mr_yCZyR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CSVLoader(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom transformer to load specific CSV files into pandas DataFrames\n",
        "    and return them in a format suitable for scikit-learn pipelines (e.g., as a tuple).\n",
        "\n",
        "    Assumes that the input X is a list of file paths, typically produced\n",
        "    by a previous step in the pipeline.\n",
        "\n",
        "    Args:\n",
        "        x_filename (str): The base name of the file containing features (e.g., 'X_train.csv').\n",
        "        y_filename (str): The base name of the file containing the target (e.g., 'y_train.csv').\n",
        "    \"\"\"\n",
        "    def __init__(self, x_filename='X_train.csv', y_filename='y_train.csv'):\n",
        "        self.x_filename = x_filename\n",
        "        self.y_filename = y_filename\n",
        "        self.x_data = None\n",
        "        self.y_data = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fits the transformer. This method will load the data.\n",
        "\n",
        "        Args:\n",
        "            X: A list of file paths (expected to contain x_filename and y_filename).\n",
        "            y: Target data (ignored).\n",
        "\n",
        "        Returns:\n",
        "            self: The fitted transformer instance.\n",
        "        \"\"\"\n",
        "        x_file_path = None\n",
        "        y_file_path = None\n",
        "\n",
        "        # Find the correct file paths in the input list\n",
        "        for file_path in X:\n",
        "            if os.path.basename(file_path) == self.x_filename:\n",
        "                x_file_path = file_path\n",
        "            elif os.path.basename(file_path) == self.y_filename:\n",
        "                y_file_path = file_path\n",
        "\n",
        "        if x_file_path is None:\n",
        "            raise FileNotFoundError(f\"Could not find {self.x_filename} in the provided file list.\")\n",
        "        if y_file_path is None:\n",
        "             raise FileNotFoundError(f\"Could not find {self.y_filename} in the provided file list.\")\n",
        "\n",
        "        try:\n",
        "            print(f\"Loading {x_file_path} into x_data...\")\n",
        "            self.x_data = pd.read_csv(x_file_path)\n",
        "            print(f\"Loading {y_file_path} into y_data...\")\n",
        "            self.y_data = pd.read_csv(y_file_path) # Or read_fwf depending on the format\n",
        "            print(\"Data loading complete.\")\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Error loading file: {e}\")\n",
        "            # You might want to re-raise the exception or handle it differently\n",
        "            raise\n",
        "        except pd.errors.EmptyDataError:\n",
        "            print(f\"Error: One of the files ({self.x_filename} or {self.y_filename}) is empty.\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred while loading data: {e}\")\n",
        "            raise\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Returns the loaded data (x_data, y_data).\n",
        "\n",
        "        Args:\n",
        "            X: Input data (ignored, data is loaded in fit).\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing (x_data, y_data) as pandas DataFrames.\n",
        "        \"\"\"\n",
        "        if self.x_data is None or self.y_data is None:\n",
        "             raise RuntimeError(\"Data has not been loaded yet. Call fit() first.\")\n",
        "\n",
        "        # Return the data in a format that can be passed to the next pipeline step\n",
        "        # For scikit-learn estimators, the fit method usually expects X and y separately.\n",
        "        # Returning a tuple (X, y) allows the next step's fit method to receive them.\n",
        "        return (self.x_data, self.y_data)\n",
        "\n",
        "# Add the CSVLoader to the pipeline\n",
        "pipeline_with_loading = Pipeline([\n",
        "    ('download_and_extract', downloader_extractor),\n",
        "    ('load_csv', CSVLoader(x_filename='X_train.csv', y_filename='y_train.csv'))\n",
        "    # Add more steps here, e.g., preprocessing, model training\n",
        "])\n",
        "\n",
        "# Run the pipeline\n",
        "# The output of the 'load_csv' step will be a tuple (x_data, y_data)\n",
        "loaded_data = pipeline_with_loading.fit_transform(None)\n",
        "\n",
        "# Access the loaded data\n",
        "x_data, y_data = loaded_data\n",
        "\n",
        "print(\"\\nLoaded x_data shape:\", x_data.shape)\n",
        "print(\"Loaded y_data shape:\", y_data.shape)\n",
        "print(\"\\nFirst 5 rows of x_data:\")\n",
        "print(x_data.head())\n",
        "print(\"\\nFirst 5 rows of y_data:\")\n",
        "print(y_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVB_ziOyaymE",
        "outputId": "b6e4d175-b2b9-4a1c-a557-b00cd38122d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://phm-datasets.s3.amazonaws.com/Data_Challenge_PHM2024_training_data.zip...\n",
            "Extracted files from https://phm-datasets.s3.amazonaws.com/Data_Challenge_PHM2024_training_data.zip to downloaded_data\n",
            "Download and extraction complete.\n",
            "CSV files found: ['downloaded_data/X_train.csv', 'downloaded_data/y_train.csv']\n",
            "Loading downloaded_data/X_train.csv into x_data...\n",
            "Loading downloaded_data/y_train.csv into y_data...\n",
            "Data loading complete.\n",
            "\n",
            "Loaded x_data shape: (742625, 8)\n",
            "Loaded y_data shape: (742625, 3)\n",
            "\n",
            "First 5 rows of x_data:\n",
            "   id  trq_measured       oat       mgt         pa       ias         np  \\\n",
            "0   0        54.100   2.00000  544.5000   212.1408  74.56250   89.18000   \n",
            "1   1        49.625  24.22231  578.4844  1625.6400  30.35596   99.55273   \n",
            "2   2        52.000   7.00000  566.1000  1912.9250  65.62500  100.14000   \n",
            "3   3        62.400   7.25000  560.1000   277.0632  54.81250   90.64000   \n",
            "4   4        62.900  23.25000  593.7000    53.6448  73.43750   99.91000   \n",
            "\n",
            "         ng  \n",
            "0   99.6400  \n",
            "1   91.3866  \n",
            "2   90.9600  \n",
            "3  100.2800  \n",
            "4   92.1700  \n",
            "\n",
            "First 5 rows of y_data:\n",
            "   id  faulty  trq_margin\n",
            "0   0       1  -13.717745\n",
            "1   1       0    1.791863\n",
            "2   2       1  -13.944871\n",
            "3   3       0   -0.017281\n",
            "4   4       0    7.322404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creazione train-set e test-set"
      ],
      "metadata": {
        "id": "7DrzNQ6kbh4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class DataPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom transformer to preprocess the data:\n",
        "    - Drop the 'id' column from x_data.\n",
        "    - Merge the 'faulty' column from y_data into x_data.\n",
        "    - Split the merged data into training and testing sets.\n",
        "\n",
        "    Assumes the input is a tuple (x_data, y_data) as pandas DataFrames,\n",
        "    typically from a previous pipeline step like CSVLoader.\n",
        "    \"\"\"\n",
        "    def __init__(self, test_size=0.2, random_state=None):\n",
        "        self.test_size = test_size\n",
        "        self.random_state = random_state\n",
        "        self.data_train = None\n",
        "        self.data_test = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fits the transformer by preprocessing and splitting the data.\n",
        "\n",
        "        Args:\n",
        "            X: A tuple (x_data, y_data) where x_data is the features DataFrame\n",
        "               and y_data is the target DataFrame.\n",
        "            y: Target data (ignored, as the target is expected in y_data).\n",
        "\n",
        "        Returns:\n",
        "            self: The fitted transformer instance.\n",
        "        \"\"\"\n",
        "        if not isinstance(X, tuple) or len(X) != 2:\n",
        "            raise TypeError(\"Input X must be a tuple (x_data, y_data).\")\n",
        "\n",
        "        x_data, y_data = X\n",
        "\n",
        "        if not isinstance(x_data, pd.DataFrame) or not isinstance(y_data, pd.DataFrame):\n",
        "             raise TypeError(\"Both elements in the input tuple must be pandas DataFrames.\")\n",
        "\n",
        "        # Drop the 'id' column from x_data if it exists\n",
        "        if 'id' in x_data.columns:\n",
        "            print(\"Dropping 'id' column from x_data...\")\n",
        "            x_data_processed = x_data.drop('id', axis=1)\n",
        "        else:\n",
        "            print(\"'id' column not found in x_data. Skipping drop.\")\n",
        "            x_data_processed = x_data.copy()\n",
        "\n",
        "        # Check if 'faulty' column exists in y_data and merge it\n",
        "        if 'faulty' in y_data.columns:\n",
        "            print(\"Merging 'faulty' column from y_data into x_data...\")\n",
        "            # Ensure dataframes can be merged, e.g., they have a common index or column\n",
        "            # Assuming they can be concatenated side-by-side based on index\n",
        "            # If merging by a specific column is needed, adjust here\n",
        "            merged_data = pd.concat([x_data_processed, y_data['faulty']], axis=1)\n",
        "        else:\n",
        "             raise ValueError(\"'faulty' column not found in y_data.\")\n",
        "\n",
        "        print(f\"Splitting data into train ({1-self.test_size:.0%}) and test ({self.test_size:.0%})...\")\n",
        "        # Split the merged data into training and testing sets\n",
        "        self.data_train, self.data_test = train_test_split(\n",
        "            merged_data,\n",
        "            test_size=self.test_size,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "        print(\"Data splitting complete.\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Returns the split training and testing data.\n",
        "\n",
        "        Args:\n",
        "            X: Input data (ignored, splitting is done in fit).\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing (data_train, data_test) as pandas DataFrames.\n",
        "        \"\"\"\n",
        "        if self.data_train is None or self.data_test is None:\n",
        "             raise RuntimeError(\"Data has not been preprocessed or split yet. Call fit() first.\")\n",
        "\n",
        "        # Return the split data\n",
        "        return (self.data_train, self.data_test)\n",
        "\n",
        "# Extend the existing pipeline to include the DataPreprocessor\n",
        "pipeline_with_preprocessing = Pipeline([\n",
        "    ('download_and_extract', downloader_extractor),\n",
        "    ('load_csv', CSVLoader(x_filename='X_train.csv', y_filename='y_train.csv')),\n",
        "    ('preprocess_and_split', DataPreprocessor(test_size=0.2, random_state=42)) # Add the preprocessor\n",
        "    # Add more steps here, e.g., feature scaling, model training\n",
        "])\n",
        "\n",
        "# Run the pipeline\n",
        "# The output of the 'preprocess_and_split' step will be a tuple (data_train, data_test)\n",
        "split_data = pipeline_with_preprocessing.fit_transform(None)\n",
        "\n",
        "# Access the split data\n",
        "data_train, data_test = split_data\n",
        "\n",
        "print(\"\\nProcessed and Split Data:\")\n",
        "print(\"data_train shape:\", data_train.shape)\n",
        "print(\"data_test shape:\", data_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cjdq-1HpbnNY",
        "outputId": "ded7fb40-b343-4d6a-e4d4-83bc19f7ff5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://phm-datasets.s3.amazonaws.com/Data_Challenge_PHM2024_training_data.zip...\n",
            "Extracted files from https://phm-datasets.s3.amazonaws.com/Data_Challenge_PHM2024_training_data.zip to downloaded_data\n",
            "Download and extraction complete.\n",
            "CSV files found: ['downloaded_data/X_train.csv', 'downloaded_data/y_train.csv']\n",
            "Loading downloaded_data/X_train.csv into x_data...\n",
            "Loading downloaded_data/y_train.csv into y_data...\n",
            "Data loading complete.\n",
            "Dropping 'id' column from x_data...\n",
            "Merging 'faulty' column from y_data into x_data...\n",
            "Splitting data into train (80%) and test (20%)...\n",
            "Data splitting complete.\n",
            "\n",
            "Processed and Split Data:\n",
            "data_train shape: (594100, 8)\n",
            "data_test shape: (148525, 8)\n"
          ]
        }
      ]
    }
  ]
}