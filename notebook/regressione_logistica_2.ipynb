{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook per regressione lineare\n",
        "\n",
        "Di seguito:\n",
        "- verr√† implementato l'algoritmo di regressione logistica"
      ],
      "metadata": {
        "id": "CIs0Jhd77joY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaricamento dei dati"
      ],
      "metadata": {
        "id": "dMd2ihQ1ZpS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "class DataDownloaderExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom transformer to download and extract data from given URLs.\n",
        "\n",
        "    Args:\n",
        "        urls (list): A list of URLs to zip files.\n",
        "        output_dir (str): The directory to save the extracted files.\n",
        "    \"\"\"\n",
        "    def __init__(self, urls, output_dir=\"dataset\"):\n",
        "        self.urls = urls\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fits the transformer. In this case, it's a no-op as there's nothing to fit.\n",
        "\n",
        "        Args:\n",
        "            X: Input data (ignored).\n",
        "            y: Target data (ignored).\n",
        "\n",
        "        Returns:\n",
        "            self: The fitted transformer instance.\n",
        "        \"\"\"\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Downloads and extracts data from the provided URLs.\n",
        "\n",
        "        Args:\n",
        "            X: Input data (ignored).\n",
        "\n",
        "        Returns:\n",
        "            list: A list of paths to the extracted CSV files.\n",
        "        \"\"\"\n",
        "        for url in self.urls:\n",
        "            try:\n",
        "                print(f\"Downloading {url}...\")\n",
        "                response = requests.get(url, stream=True)\n",
        "                response.raise_for_status() # Raise an exception for bad status codes\n",
        "\n",
        "                # Read the zip file from the response content\n",
        "                with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
        "                    # Extract all contents to the specified output directory\n",
        "                    zip_ref.extractall(self.output_dir)\n",
        "                    print(f\"Extracted files from {url} to {self.output_dir}\")\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error downloading {url}: {e}\")\n",
        "            except zipfile.BadZipFile:\n",
        "                print(f\"Error: The downloaded file from {url} is not a valid zip file.\")\n",
        "            except Exception as e:\n",
        "                print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "        print(\"Download and extraction complete.\")\n",
        "        csv_files = glob.glob(os.path.join(self.output_dir, \"*.csv\"))\n",
        "        print(\"CSV files found:\", csv_files)\n",
        "        return csv_files\n",
        "\n",
        "# Example usage with a scikit-learn pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# List of URLs to your zipped files on AWS\n",
        "urls = [\n",
        "    \"https://phm-datasets.s3.amazonaws.com/Data_Challenge_PHM2024_training_data.zip\",\n",
        "]\n",
        "\n",
        "# Create the custom transformer\n",
        "downloader_extractor = DataDownloaderExtractor(urls=urls, output_dir=\"downloaded_data\")\n",
        "\n",
        "# You can add other steps to the pipeline if needed, e.g., a data loader\n",
        "# For this example, we just have the download and extract step\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('download_and_extract', downloader_extractor),\n",
        "    # Add more steps here if needed, e.g., loading the CSV files into pandas DataFrames\n",
        "])\n",
        "\n",
        "# Run the pipeline\n",
        "# The fit method is called first (though it does nothing in this transformer)\n",
        "# Then the transform method is called to perform the download and extraction\n",
        "extracted_files = pipeline.fit_transform(None) # Pass None as input data, as it's not used\n",
        "\n",
        "print(\"\\nPipeline execution complete.\")\n",
        "print(\"Extracted files:\", extracted_files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KabvZ7YaKIx",
        "outputId": "6d6e58fe-af15-4add-d4b8-212d3d9aad12"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://phm-datasets.s3.amazonaws.com/Data_Challenge_PHM2024_training_data.zip...\n",
            "Extracted files from https://phm-datasets.s3.amazonaws.com/Data_Challenge_PHM2024_training_data.zip to downloaded_data\n",
            "Download and extraction complete.\n",
            "CSV files found: ['downloaded_data/X_train.csv', 'downloaded_data/y_train.csv']\n",
            "\n",
            "Pipeline execution complete.\n",
            "Extracted files: ['downloaded_data/X_train.csv', 'downloaded_data/y_train.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estrazione dei dati"
      ],
      "metadata": {
        "id": "er3mr_yCZyR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CSVLoader(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom transformer to load specific CSV files into pandas DataFrames\n",
        "    and return them in a format suitable for scikit-learn pipelines (e.g., as a tuple).\n",
        "\n",
        "    Assumes that the input X is a list of file paths, typically produced\n",
        "    by a previous step in the pipeline.\n",
        "\n",
        "    Args:\n",
        "        x_filename (str): The base name of the file containing features (e.g., 'X_train.csv').\n",
        "        y_filename (str): The base name of the file containing the target (e.g., 'y_train.csv').\n",
        "    \"\"\"\n",
        "    def __init__(self, x_filename='X_train.csv', y_filename='y_train.csv'):\n",
        "        self.x_filename = x_filename\n",
        "        self.y_filename = y_filename\n",
        "        self.x_data = None\n",
        "        self.y_data = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fits the transformer. This method will load the data.\n",
        "\n",
        "        Args:\n",
        "            X: A list of file paths (expected to contain x_filename and y_filename).\n",
        "            y: Target data (ignored).\n",
        "\n",
        "        Returns:\n",
        "            self: The fitted transformer instance.\n",
        "        \"\"\"\n",
        "        x_file_path = None\n",
        "        y_file_path = None\n",
        "\n",
        "        # Find the correct file paths in the input list\n",
        "        for file_path in X:\n",
        "            if os.path.basename(file_path) == self.x_filename:\n",
        "                x_file_path = file_path\n",
        "            elif os.path.basename(file_path) == self.y_filename:\n",
        "                y_file_path = file_path\n",
        "\n",
        "        if x_file_path is None:\n",
        "            raise FileNotFoundError(f\"Could not find {self.x_filename} in the provided file list.\")\n",
        "        if y_file_path is None:\n",
        "             raise FileNotFoundError(f\"Could not find {self.y_filename} in the provided file list.\")\n",
        "\n",
        "        try:\n",
        "            print(f\"Loading {x_file_path} into x_data...\")\n",
        "            self.x_data = pd.read_csv(x_file_path)\n",
        "            print(f\"Loading {y_file_path} into y_data...\")\n",
        "            self.y_data = pd.read_csv(y_file_path) # Or read_fwf depending on the format\n",
        "            print(\"Data loading complete.\")\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Error loading file: {e}\")\n",
        "            # You might want to re-raise the exception or handle it differently\n",
        "            raise\n",
        "        except pd.errors.EmptyDataError:\n",
        "            print(f\"Error: One of the files ({self.x_filename} or {self.y_filename}) is empty.\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred while loading data: {e}\")\n",
        "            raise\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Returns the loaded data (x_data, y_data).\n",
        "\n",
        "        Args:\n",
        "            X: Input data (ignored, data is loaded in fit).\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing (x_data, y_data) as pandas DataFrames.\n",
        "        \"\"\"\n",
        "        if self.x_data is None or self.y_data is None:\n",
        "             raise RuntimeError(\"Data has not been loaded yet. Call fit() first.\")\n",
        "\n",
        "        # Return the data in a format that can be passed to the next pipeline step\n",
        "        # For scikit-learn estimators, the fit method usually expects X and y separately.\n",
        "        # Returning a tuple (X, y) allows the next step's fit method to receive them.\n",
        "        return (self.x_data, self.y_data)\n",
        "\n",
        "# Add the CSVLoader to the pipeline\n",
        "pipeline_with_loading = Pipeline([\n",
        "    ('download_and_extract', downloader_extractor),\n",
        "    ('load_csv', CSVLoader(x_filename='X_train.csv', y_filename='y_train.csv'))\n",
        "    # Add more steps here, e.g., preprocessing, model training\n",
        "])\n",
        "\n",
        "# Run the pipeline\n",
        "# The output of the 'load_csv' step will be a tuple (x_data, y_data)\n",
        "loaded_data = pipeline_with_loading.fit_transform(None)\n",
        "\n",
        "# Access the loaded data\n",
        "x_data, y_data = loaded_data\n",
        "\n",
        "print(\"\\nLoaded x_data shape:\", x_data.shape)\n",
        "print(\"Loaded y_data shape:\", y_data.shape)\n",
        "print(\"\\nFirst 5 rows of x_data:\")\n",
        "print(x_data.head())\n",
        "print(\"\\nFirst 5 rows of y_data:\")\n",
        "print(y_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVB_ziOyaymE",
        "outputId": "119b8a69-7340-4143-e72c-82aaa38357e1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://phm-datasets.s3.amazonaws.com/Data_Challenge_PHM2024_training_data.zip...\n",
            "Extracted files from https://phm-datasets.s3.amazonaws.com/Data_Challenge_PHM2024_training_data.zip to downloaded_data\n",
            "Download and extraction complete.\n",
            "CSV files found: ['downloaded_data/X_train.csv', 'downloaded_data/y_train.csv']\n",
            "Loading downloaded_data/X_train.csv into x_data...\n",
            "Loading downloaded_data/y_train.csv into y_data...\n",
            "Data loading complete.\n",
            "\n",
            "Loaded x_data shape: (742625, 8)\n",
            "Loaded y_data shape: (742625, 3)\n",
            "\n",
            "First 5 rows of x_data:\n",
            "   id  trq_measured       oat       mgt         pa       ias         np  \\\n",
            "0   0        54.100   2.00000  544.5000   212.1408  74.56250   89.18000   \n",
            "1   1        49.625  24.22231  578.4844  1625.6400  30.35596   99.55273   \n",
            "2   2        52.000   7.00000  566.1000  1912.9250  65.62500  100.14000   \n",
            "3   3        62.400   7.25000  560.1000   277.0632  54.81250   90.64000   \n",
            "4   4        62.900  23.25000  593.7000    53.6448  73.43750   99.91000   \n",
            "\n",
            "         ng  \n",
            "0   99.6400  \n",
            "1   91.3866  \n",
            "2   90.9600  \n",
            "3  100.2800  \n",
            "4   92.1700  \n",
            "\n",
            "First 5 rows of y_data:\n",
            "   id  faulty  trq_margin\n",
            "0   0       1  -13.717745\n",
            "1   1       0    1.791863\n",
            "2   2       1  -13.944871\n",
            "3   3       0   -0.017281\n",
            "4   4       0    7.322404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creazione train-set e test-set"
      ],
      "metadata": {
        "id": "7DrzNQ6kbh4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class DataPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom transformer to preprocess the data:\n",
        "    - Drop the 'id' column from x_data.\n",
        "    - Merge the 'faulty' column from y_data into x_data.\n",
        "    - Split the merged data into training and testing sets.\n",
        "\n",
        "    Assumes the input is a tuple (x_data, y_data) as pandas DataFrames,\n",
        "    typically from a previous pipeline step like CSVLoader.\n",
        "    \"\"\"\n",
        "    def __init__(self, test_size=0.2, random_state=None):\n",
        "        self.test_size = test_size\n",
        "        self.random_state = random_state\n",
        "        self.data_train = None\n",
        "        self.data_test = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fits the transformer by preprocessing and splitting the data.\n",
        "\n",
        "        Args:\n",
        "            X: A tuple (x_data, y_data) where x_data is the features DataFrame\n",
        "               and y_data is the target DataFrame.\n",
        "            y: Target data (ignored, as the target is expected in y_data).\n",
        "\n",
        "        Returns:\n",
        "            self: The fitted transformer instance.\n",
        "        \"\"\"\n",
        "        if not isinstance(X, tuple) or len(X) != 2:\n",
        "            raise TypeError(\"Input X must be a tuple (x_data, y_data).\")\n",
        "\n",
        "        x_data, y_data = X\n",
        "\n",
        "        if not isinstance(x_data, pd.DataFrame) or not isinstance(y_data, pd.DataFrame):\n",
        "             raise TypeError(\"Both elements in the input tuple must be pandas DataFrames.\")\n",
        "\n",
        "        # Drop the 'id' column from x_data if it exists\n",
        "        if 'id' in x_data.columns:\n",
        "            print(\"Dropping 'id' column from x_data...\")\n",
        "            x_data_processed = x_data.drop('id', axis=1)\n",
        "        else:\n",
        "            print(\"'id' column not found in x_data. Skipping drop.\")\n",
        "            x_data_processed = x_data.copy()\n",
        "\n",
        "        # Check if 'faulty' column exists in y_data and merge it\n",
        "        if 'faulty' in y_data.columns:\n",
        "            print(\"Merging 'faulty' column from y_data into x_data...\")\n",
        "            # Ensure dataframes can be merged, e.g., they have a common index or column\n",
        "            # Assuming they can be concatenated side-by-side based on index\n",
        "            # If merging by a specific column is needed, adjust here\n",
        "            merged_data = pd.concat([x_data_processed, y_data['faulty']], axis=1)\n",
        "        else:\n",
        "             raise ValueError(\"'faulty' column not found in y_data.\")\n",
        "\n",
        "        print(f\"Splitting data into train ({1-self.test_size:.0%}) and test ({self.test_size:.0%})...\")\n",
        "        # Split the merged data into training and testing sets\n",
        "        self.data_train, self.data_test = train_test_split(\n",
        "            merged_data,\n",
        "            test_size=self.test_size,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "        print(\"Data splitting complete.\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Returns the split training and testing data.\n",
        "\n",
        "        Args:\n",
        "            X: Input data (ignored, splitting is done in fit).\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing (data_train, data_test) as pandas DataFrames.\n",
        "        \"\"\"\n",
        "        if self.data_train is None or self.data_test is None:\n",
        "             raise RuntimeError(\"Data has not been preprocessed or split yet. Call fit() first.\")\n",
        "\n",
        "        # Return the split data\n",
        "        return (self.data_train, self.data_test)\n",
        "\n",
        "# Extend the existing pipeline to include the DataPreprocessor\n",
        "pipeline_with_preprocessing = Pipeline([\n",
        "    ('download_and_extract', downloader_extractor),\n",
        "    ('load_csv', CSVLoader(x_filename='X_train.csv', y_filename='y_train.csv')),\n",
        "    ('preprocess_and_split', DataPreprocessor(test_size=0.2, random_state=42)) # Add the preprocessor\n",
        "    # Add more steps here, e.g., feature scaling, model training\n",
        "])\n",
        "\n",
        "# Run the pipeline\n",
        "# The output of the 'preprocess_and_split' step will be a tuple (data_train, data_test)\n",
        "split_data = pipeline_with_preprocessing.fit_transform(None)\n",
        "\n",
        "# Access the split data\n",
        "data_train, data_test = split_data\n",
        "\n",
        "print(\"\\nProcessed and Split Data:\")\n",
        "print(\"data_train shape:\", data_train.shape)\n",
        "print(\"data_test shape:\", data_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cjdq-1HpbnNY",
        "outputId": "f835ad2d-fbd6-4632-8729-432b3c9aa591"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://phm-datasets.s3.amazonaws.com/Data_Challenge_PHM2024_training_data.zip...\n",
            "Extracted files from https://phm-datasets.s3.amazonaws.com/Data_Challenge_PHM2024_training_data.zip to downloaded_data\n",
            "Download and extraction complete.\n",
            "CSV files found: ['downloaded_data/X_train.csv', 'downloaded_data/y_train.csv']\n",
            "Loading downloaded_data/X_train.csv into x_data...\n",
            "Loading downloaded_data/y_train.csv into y_data...\n",
            "Data loading complete.\n",
            "Dropping 'id' column from x_data...\n",
            "Merging 'faulty' column from y_data into x_data...\n",
            "Splitting data into train (80%) and test (20%)...\n",
            "Data splitting complete.\n",
            "\n",
            "Processed and Split Data:\n",
            "data_train shape: (594100, 8)\n",
            "data_test shape: (148525, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful package\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def transform_with_custom_root(df, column_name, root_degree):\n",
        "  \"\"\"\n",
        "  Applies a custom root transformation (1/root_degree power) to a column.\n",
        "  Handles positive, negative, and zero values appropriately based on the root degree.\n",
        "\n",
        "  Args:\n",
        "    df (pd.DataFrame): The input DataFrame.\n",
        "    column_name (str): The name of the column to transform.\n",
        "    root_degree (float): The degree of the root (e.g., 2 for square root, 3 for cube root).\n",
        "\n",
        "  Returns:\n",
        "    pd.DataFrame: The DataFrame with the transformed column.\n",
        "  \"\"\"\n",
        "  new_column_name = f'{column_name}_root_{root_degree:.2f}_transformed'\n",
        "\n",
        "  if root_degree == 0:\n",
        "      raise ValueError(\"Root degree cannot be zero.\")\n",
        "  elif root_degree % 2 == 0:  # Even root\n",
        "      # For even roots, we can only take the root of non-negative numbers\n",
        "      if (df[column_name] < 0).any():\n",
        "          print(f\"Warning: Column '{column_name}' contains negative values. Cannot apply even root directly.\")\n",
        "          # You might choose to handle this by taking the root of the absolute value,\n",
        "          # or setting negative values to NaN, depending on your data context.\n",
        "          # Here, we'll take the root of the absolute value for demonstration.\n",
        "          df[new_column_name] = np.power(np.abs(df[column_name]), 1/root_degree)\n",
        "      else:\n",
        "          df[new_column_name] = np.power(df[column_name], 1/root_degree)\n",
        "  else:  # Odd root\n",
        "      # Odd roots can handle positive, negative, and zero values\n",
        "      df[new_column_name] = np.sign(df[column_name]) * np.power(np.abs(df[column_name]), 1/root_degree)\n",
        "\n",
        "  return df\n",
        "\n",
        "# Example usage with a custom root (e.g., 1.5)\n",
        "# custom_root_degree = 2.35\n",
        "# data_train = transform_with_custom_root(data_train.copy(), 'power_avail', custom_root_degree)\n",
        "\n",
        "\n",
        "def create_binned_qualitative_variable(df, column_name, num_bins, strategy='quantile'):\n",
        "  \"\"\"\n",
        "  Creates a qualitative (categorical) variable by binning a numerical column.\n",
        "\n",
        "  Args:\n",
        "    df (pd.DataFrame): The input DataFrame.\n",
        "    column_name (str): The name of the numerical column to bin.\n",
        "    num_bins (int): The desired number of bins.\n",
        "    strategy (str): The strategy to use for binning. 'quantile' uses quantiles\n",
        "                    to ensure bins have approximately equal numbers of observations.\n",
        "                    'uniform' creates bins with equal widths. Default is 'quantile'.\n",
        "\n",
        "  Returns:\n",
        "    pd.DataFrame: The DataFrame with a new qualitative column.\n",
        "                  The new column name will be f'{column_name}_binned_{num_bins}_{strategy}'.\n",
        "  \"\"\"\n",
        "  if column_name not in df.columns:\n",
        "    raise ValueError(f\"La colonna '{column_name}' non √® presente nel DataFrame.\")\n",
        "  if num_bins <= 1:\n",
        "      raise ValueError(\"Il numero di bins deve essere maggiore di 1.\")\n",
        "\n",
        "  new_column_name = f'{column_name}_binned_{num_bins}_{strategy}'\n",
        "\n",
        "  if strategy == 'quantile':\n",
        "    # Use qcut to create bins based on quantiles (approximately equal number of observations)\n",
        "    # `duplicates='drop'` handles cases where quantile boundaries are not unique,\n",
        "    # which can happen with skewed or discrete data.\n",
        "    df[new_column_name] = pd.qcut(df[column_name], q=num_bins, labels=False, duplicates='drop')\n",
        "  elif strategy == 'uniform':\n",
        "    # Use cut to create bins of equal width\n",
        "    df[new_column_name] = pd.cut(df[column_name], bins=num_bins, labels=False, include_lowest=True)\n",
        "  else:\n",
        "    raise ValueError(f\"Strategia di binning non valida: '{strategy}'. Scegliere tra 'quantile' o 'uniform'.\")\n",
        "\n",
        "  # Convert the binned column to object/category type if needed, or keep as int for simplicity\n",
        "  # Here we keep it as int representing the bin number\n",
        "\n",
        "  return df\n",
        "\n",
        "# Example usage for 'indicated_air_speed':\n",
        "# num_bins_indicated_air_speed = 5 # Define the number of bins\n",
        "# binning_strategy = 'quantile' # Or 'uniform'\n",
        "\n",
        "#data_train = create_binned_qualitative_variable(\n",
        "#    data_train.copy(),\n",
        "#    'indicated_air_speed',\n",
        "#    num_bins_indicated_air_speed,\n",
        "#    strategy=binning_strategy\n",
        "#)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## PCA per indicated_air_speed e compressor_speed\n",
        "# Select the columns for PCA\n",
        "# features_for_pca = data_train[['compressor_speed', 'net_power']]\n",
        "# Initialize PCA with 1 component (to combine the two variables)\n",
        "# pca = PCA(n_components=1)\n",
        "# Fit PCA on the selected features and transform them\n",
        "# data_train['compressor_speed_net_power_pca'] = pca.fit_transform(features_for_pca)\n",
        "\n",
        "\n",
        "\n",
        "## Creazione di torque_times_temp\n",
        "\n",
        "# data_train['torque_times_temp'] = data_train['torque_meas'] * data_train['outside_air_temp']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Creazione pipeline\n",
        "def prepare_data_pipeline(x_path, y_path, new_column_names=None,\n",
        "                          root_transformations=None,\n",
        "                          binning_config=None,\n",
        "                          standardize=True,\n",
        "                          drop_index_col='idx'):\n",
        "    \"\"\"\n",
        "    Esegue la pipeline completa di preprocessing.\n",
        "\n",
        "    Args:\n",
        "        x_path (str): path al file X_train.csv\n",
        "        y_path (str): path al file y_train.csv\n",
        "        new_column_names (list): lista di nuovi nomi colonne (opzionale)\n",
        "        root_transformations (dict): dict {colonna: radice}\n",
        "        binning_config (dict): dict {colonna: (num_bins, strategia)}\n",
        "        standardize (bool): se standardizzare le colonne numeriche\n",
        "        drop_index_col (str): nome della colonna da droppare (opzionale)\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame preprocessato pronto per il training\n",
        "    \"\"\"\n",
        "    df = load_training_data(x_path, y_path)\n",
        "\n",
        "    if new_column_names:\n",
        "        df = rename_dataframe_columns(df, new_column_names + ['y_target'])\n",
        "\n",
        "    if drop_index_col in df.columns:\n",
        "        df = df.drop(drop_index_col, axis=1)\n",
        "\n",
        "    # Trasformazioni custom root\n",
        "    if root_transformations:\n",
        "        for col, deg in root_transformations.items():\n",
        "            df = transform_with_custom_root(df, col, deg)\n",
        "\n",
        "    # Binning\n",
        "    if binning_config:\n",
        "        for col, (n_bins, strategy) in binning_config.items():\n",
        "            df = create_binned_qualitative_variable(df, col, n_bins, strategy)\n",
        "\n",
        "    # PCA: esempio hardcoded ma puoi parametrizzare se vuoi\n",
        "    if {'compressor_speed', 'net_power'}.issubset(df.columns):\n",
        "        pca = PCA(n_components=1)\n",
        "        df['compressor_speed_net_power_pca'] = pca.fit_transform(df[['compressor_speed', 'net_power']])\n",
        "\n",
        "    # Feature engineering manuale\n",
        "    if {'torque_meas', 'outside_air_temp'}.issubset(df.columns):\n",
        "        df['torque_times_temp'] = df['torque_meas'] * df['outside_air_temp']\n",
        "\n",
        "    # Rimuovi colonne non necessarie\n",
        "    columns_to_drop = ['compressor_speed','net_power','indicated_air_speed','power_avail']  # Aggiungi qui altre colonne da rimuovere\n",
        "    df = df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "    # Assicurati che 'y_target' sia l'ultima colonna\n",
        "    if 'y_target' in df.columns:\n",
        "        cols = [col for col in df.columns if col != 'y_target'] + ['y_target']\n",
        "        df = df[cols]\n",
        "    else:\n",
        "        print(\"Warning: 'y_target' column not found in DataFrame. It will not be moved to the end.\")\n",
        "\n",
        "    # Assicurati che il DataFrame non abbia colonne duplicate\n",
        "    df = df.loc[:, ~df.columns.duplicated()]\n",
        "\n",
        "    # Assicurati che il DataFrame non abbia valori NaN\n",
        "    if df.isnull().values.any():\n",
        "        print(\"Warning: DataFrame contains NaN values. They will be filled with 0.\")\n",
        "        df = df.fillna(0)\n",
        "\n",
        "    # Assicurati che il DataFrame non abbia valori infiniti\n",
        "    if np.isinf(df.values).any():\n",
        "        print(\"Warning: DataFrame contains infinite values. They will be replaced with 0.\")\n",
        "        df.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "\n",
        "    # Standardizzazione\n",
        "    if standardize:\n",
        "        target = 'y_target'\n",
        "        numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "        columns_to_standardize = [col for col in numerical_cols if col != target]\n",
        "        df = standardize_columns(df, columns_to_standardize)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "### Esempio di utilizzo della pipeline\n",
        "x_path = 'dataset/X_data.csv'\n",
        "y_path = 'dataset/y_data.csv'\n",
        "\n",
        "# Configurazioni opzionali\n",
        "new_column_names = ['idx', 'torque_meas', 'outside_air_temp', 'mean_gas_temp',\n",
        "                    'power_avail', 'indicated_air_speed', 'net_power', 'compressor_speed']\n",
        "\n",
        "root_transform = {'power_avail': 2.35}\n",
        "binning = {'indicated_air_speed': (5, 'quantile')}\n",
        "\n",
        "data_ready = prepare_data_pipeline(\n",
        "    x_path, y_path,\n",
        "    new_column_names=new_column_names,\n",
        "    root_transformations=root_transform,\n",
        "    binning_config=binning\n",
        ")\n",
        "\n",
        "# Esempio di stampa del DataFrame preprocessato\n",
        "print(data_ready.head())\n",
        "print(data_ready.describe())\n",
        "\n",
        "n_cols = 3\n",
        "n_rows = (len(data_ready.columns) + n_cols - 1) // n_cols\n",
        "plt.figure(figsize=(15, n_rows * 4))\n",
        "for i, col in enumerate(data_ready):\n",
        "    plt.subplot(n_rows, n_cols, i + 1)\n",
        "    sns.histplot(data_ready[col], bins=50, kde=True)\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"grafico.png\")\n",
        "\n",
        "# data_ready ora √® pronto per essere usato in un modello"
      ],
      "metadata": {
        "id": "SYD6QZ_spo-d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "4d9a52bb-a141-472f-ddbb-d6a2b12d19a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_training_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ed17a16c8fe3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0mbinning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'indicated_air_speed'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'quantile'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m data_ready = prepare_data_pipeline(\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mx_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mnew_column_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_column_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-ed17a16c8fe3>\u001b[0m in \u001b[0;36mprepare_data_pipeline\u001b[0;34m(x_path, y_path, new_column_names, root_transformations, binning_config, standardize, drop_index_col)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m \u001b[0mpreprocessato\u001b[0m \u001b[0mpronto\u001b[0m \u001b[0mper\u001b[0m \u001b[0mil\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \"\"\"\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_column_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_training_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Radice custom"
      ],
      "metadata": {
        "id": "ZSURPvKHuL8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trasformazione in work_with_data"
      ],
      "metadata": {
        "id": "0tSW1aPUtxV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_with_custom_root(df, column_name, root_degree):\n",
        "  \"\"\"\n",
        "  Applies a custom root transformation (1/root_degree power) to a column.\n",
        "  Handles positive, negative, and zero values appropriately based on the root degree.\n",
        "\n",
        "  Args:\n",
        "    df (pd.DataFrame): The input DataFrame.\n",
        "    column_name (str): The name of the column to transform.\n",
        "    root_degree (float): The degree of the root (e.g., 2 for square root, 3 for cube root).\n",
        "\n",
        "  Returns:\n",
        "    pd.DataFrame: The DataFrame with the transformed column.\n",
        "  \"\"\"\n",
        "  new_column_name = f'{column_name}_root_{root_degree:.2f}_transformed'\n",
        "\n",
        "  if root_degree == 0:\n",
        "      raise ValueError(\"Root degree cannot be zero.\")\n",
        "  elif root_degree % 2 == 0:  # Even root\n",
        "      # For even roots, we can only take the root of non-negative numbers\n",
        "      if (df[column_name] < 0).any():\n",
        "          print(f\"Warning: Column '{column_name}' contains negative values. Cannot apply even root directly.\")\n",
        "          # You might choose to handle this by taking the root of the absolute value,\n",
        "          # or setting negative values to NaN, depending on your data context.\n",
        "          # Here, we'll take the root of the absolute value for demonstration.\n",
        "          df[new_column_name] = np.power(np.abs(df[column_name]), 1/root_degree)\n",
        "      else:\n",
        "          df[new_column_name] = np.power(df[column_name], 1/root_degree)\n",
        "  else:  # Odd root\n",
        "      # Odd roots can handle positive, negative, and zero values\n",
        "      df[new_column_name] = np.sign(df[column_name]) * np.power(np.abs(df[column_name]), 1/root_degree)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "_UAxC3Cosvjm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trasformazione pipeline"
      ],
      "metadata": {
        "id": "xnmnqu9lwe61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crea una classe che sia compatibile con l'oggetto pipeline di scikit che contenga la funzione transform_with_custom_root\n",
        "\n",
        "class CustomRootTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom transformer to apply a custom root transformation to specified columns.\n",
        "    Compatible with scikit-learn pipelines.\n",
        "\n",
        "    Args:\n",
        "        root_transformations (dict): A dictionary where keys are column names\n",
        "                                     and values are the root degrees to apply.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_transformations=None):\n",
        "        self.root_transformations = root_transformations\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fits the transformer. In this case, there's nothing to fit.\n",
        "\n",
        "        Args:\n",
        "            X: Input data (ignored).\n",
        "            y: Target data (ignored).\n",
        "\n",
        "        Returns:\n",
        "            self: The fitted transformer instance.\n",
        "        \"\"\"\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Applies the custom root transformation to the specified columns in the DataFrame.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): The input DataFrame.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: The DataFrame with the transformed columns.\n",
        "        \"\"\"\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
        "\n",
        "        df_transformed = X.copy() # Work on a copy to avoid modifying the original DataFrame\n",
        "\n",
        "        if self.root_transformations:\n",
        "            for col, deg in self.root_transformations.items():\n",
        "                if col not in df_transformed.columns:\n",
        "                    print(f\"Warning: Column '{col}' not found in DataFrame. Skipping root transformation.\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"Applying custom root {deg} transformation to column '{col}'...\")\n",
        "                df_transformed = transform_with_custom_root(df_transformed, col, deg)\n",
        "\n",
        "        return df_transformed\n",
        "\n"
      ],
      "metadata": {
        "id": "C0HoEko5tT-V"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binary transformation"
      ],
      "metadata": {
        "id": "CKNFnVhcuRiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trasformazione work_with_data"
      ],
      "metadata": {
        "id": "teNr1y8MwUHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_binned_qualitative_variable(df, column_name, num_bins, strategy='quantile'):\n",
        "  \"\"\"\n",
        "  Creates a qualitative (categorical) variable by binning a numerical column.\n",
        "\n",
        "  Args:\n",
        "    df (pd.DataFrame): The input DataFrame.\n",
        "    column_name (str): The name of the numerical column to bin.\n",
        "    num_bins (int): The desired number of bins.\n",
        "    strategy (str): The strategy to use for binning. 'quantile' uses quantiles\n",
        "                    to ensure bins have approximately equal numbers of observations.\n",
        "                    'uniform' creates bins with equal widths. Default is 'quantile'.\n",
        "\n",
        "  Returns:\n",
        "    pd.DataFrame: The DataFrame with a new qualitative column.\n",
        "                  The new column name will be f'{column_name}_binned_{num_bins}_{strategy}'.\n",
        "  \"\"\"\n",
        "  if column_name not in df.columns:\n",
        "    raise ValueError(f\"La colonna '{column_name}' non √® presente nel DataFrame.\")\n",
        "  if num_bins <= 1:\n",
        "      raise ValueError(\"Il numero di bins deve essere maggiore di 1.\")\n",
        "\n",
        "  new_column_name = f'{column_name}_binned_{num_bins}_{strategy}'\n",
        "\n",
        "  if strategy == 'quantile':\n",
        "    # Use qcut to create bins based on quantiles (approximately equal number of observations)\n",
        "    # `duplicates='drop'` handles cases where quantile boundaries are not unique,\n",
        "    # which can happen with skewed or discrete data.\n",
        "    df[new_column_name] = pd.qcut(df[column_name], q=num_bins, labels=False, duplicates='drop')\n",
        "  elif strategy == 'uniform':\n",
        "    # Use cut to create bins of equal width\n",
        "    df[new_column_name] = pd.cut(df[column_name], bins=num_bins, labels=False, include_lowest=True)\n",
        "  else:\n",
        "    raise ValueError(f\"Strategia di binning non valida: '{strategy}'. Scegliere tra 'quantile' o 'uniform'.\")\n",
        "\n",
        "  # Convert the binned column to object/category type if needed, or keep as int for simplicity\n",
        "  # Here we keep it as int representing the bin number\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "eSLqiujBulI1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trasformazione pipeline"
      ],
      "metadata": {
        "id": "ajBPOBOJutTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crea una classe che sia compatibile con l'oggetto pipeline di scikit che contenga la funzione  create_binned_qualitative_variable\n",
        "\n",
        "class BinnedQualitativeTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom transformer to create binned qualitative (categorical) variables\n",
        "    from numerical columns. Compatible with scikit-learn pipelines.\n",
        "\n",
        "    Args:\n",
        "        binning_config (dict): A dictionary where keys are column names\n",
        "                               and values are tuples (num_bins, strategy).\n",
        "                               Strategy can be 'quantile' or 'uniform'.\n",
        "    \"\"\"\n",
        "    def __init__(self, binning_config=None):\n",
        "        self.binning_config = binning_config\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fits the transformer. In this case, there's nothing to fit.\n",
        "\n",
        "        Args:\n",
        "            X: Input data (ignored).\n",
        "            y: Target data (ignored).\n",
        "\n",
        "        Returns:\n",
        "            self: The fitted transformer instance.\n",
        "        \"\"\"\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Applies the binning transformation to the specified columns in the DataFrame.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): The input DataFrame.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: The DataFrame with the new binned qualitative columns.\n",
        "        \"\"\"\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
        "\n",
        "        df_transformed = X.copy() # Work on a copy to avoid modifying the original DataFrame\n",
        "\n",
        "        if self.binning_config:\n",
        "            for col, (num_bins, strategy) in self.binning_config.items():\n",
        "                if col not in df_transformed.columns:\n",
        "                    print(f\"Warning: Column '{col}' not found in DataFrame. Skipping binning transformation.\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"Applying binning transformation to column '{col}' with {num_bins} bins and strategy '{strategy}'...\")\n",
        "                df_transformed = create_binned_qualitative_variable(df_transformed, col, num_bins, strategy)\n",
        "\n",
        "        return df_transformed\n"
      ],
      "metadata": {
        "id": "cXcKS456u7AM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compressor speed net power"
      ],
      "metadata": {
        "id": "5t1DNnKZvv0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trasformazione con work_with_data"
      ],
      "metadata": {
        "id": "Tkns2nkBx9wC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the columns for PCA\n",
        "features_for_pca = data_train[['compressor_speed', 'net_power']]\n",
        "\n",
        "# Initialize PCA with 1 component (to combine the two variables)\n",
        "pca = PCA(n_components=1)\n",
        "\n",
        "# Fit PCA on the selected features and transform them\n",
        "data_train['compressor_speed_net_power_pca'] = pca.fit_transform(features_for_pca)"
      ],
      "metadata": {
        "id": "vvEG6L_0xDds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trasformazione pipeline"
      ],
      "metadata": {
        "id": "_upK78g00f3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crea una classe che sia compatibile con l'oggetto pipeline di scikit che  come nella cella sopra a partire dalle feature 'compressor_speed' e net_power' tramite il metodo PCA crei una nuova variabile 'compressor_speed_net_power_pca'\n",
        "\n",
        "class PCATransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom transformer to apply PCA on specified features.\n",
        "    Compatible with scikit-learn pipelines.\n",
        "\n",
        "    Args:\n",
        "        features (list): A list of column names to apply PCA on.\n",
        "        n_components (int or float or 'mle'): The number of components to keep.\n",
        "                                              Refer to sklearn.decomposition.PCA documentation.\n",
        "        new_column_name (str): The name for the new PCA component column.\n",
        "    \"\"\"\n",
        "    def __init__(self, features, n_components=1, new_column_name='pca_component'):\n",
        "        if not isinstance(features, list) or len(features) < 2:\n",
        "            raise ValueError(\"Features must be a list of at least two column names.\")\n",
        "        self.features = features\n",
        "        self.n_components = n_components\n",
        "        self.new_column_name = new_column_name\n",
        "        self.pca_ = PCA(n_components=self.n_components) # Initialize PCA model\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fits the PCA model on the specified features of the input data.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): The input DataFrame containing the features.\n",
        "            y: Target data (ignored).\n",
        "\n",
        "        Returns:\n",
        "            self: The fitted transformer instance.\n",
        "        \"\"\"\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
        "\n",
        "        # Check if all specified features exist in the DataFrame\n",
        "        if not set(self.features).issubset(X.columns):\n",
        "            missing_features = list(set(self.features) - set(X.columns))\n",
        "            raise ValueError(f\"Missing features in DataFrame: {missing_features}\")\n",
        "\n",
        "        print(f\"Fitting PCA on features: {self.features} with {self.n_components} components...\")\n",
        "        # Fit the PCA model on the selected columns\n",
        "        self.pca_.fit(X[self.features])\n",
        "        print(\"PCA fitting complete.\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Applies the fitted PCA transformation to the specified features and adds\n",
        "        the new component(s) to the DataFrame.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): The input DataFrame containing the features.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: The DataFrame with the new PCA component column(s).\n",
        "        \"\"\"\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
        "\n",
        "        # Check if all specified features exist in the DataFrame\n",
        "        if not set(self.features).issubset(X.columns):\n",
        "            missing_features = list(set(self.features) - set(X.columns))\n",
        "            raise ValueError(f\"Missing features in DataFrame: {missing_features}\")\n",
        "\n",
        "        print(f\"Transforming data using fitted PCA on features: {self.features}...\")\n",
        "        # Transform the selected columns using the fitted PCA model\n",
        "        pca_components = self.pca_.transform(X[self.features])\n",
        "\n",
        "        # Create a DataFrame for the PCA components\n",
        "        if self.n_components == 1:\n",
        "            pca_df = pd.DataFrame(pca_components, index=X.index, columns=[self.new_column_name])\n",
        "        else:\n",
        "            # If multiple components, name them accordingly\n",
        "            component_names = [f'{self.new_column_name}_{i+1}' for i in range(pca_components.shape[1])]\n",
        "            pca_df = pd.DataFrame(pca_components, index=X.index, columns=component_names)\n",
        "\n",
        "        # Concatenate the original DataFrame (excluding the original features used for PCA)\n",
        "        # with the new PCA component DataFrame.\n",
        "        # We drop the original features from the input DataFrame X before concatenating\n",
        "        # to avoid redundancy, assuming the PCA components replace them conceptually.\n",
        "        # If you want to keep the original features, remove the .drop(self.features, axis=1) part.\n",
        "        df_transformed = pd.concat([X.drop(self.features, axis=1), pca_df], axis=1)\n",
        "        print(\"PCA transformation complete.\")\n",
        "\n",
        "        return df_transformed\n"
      ],
      "metadata": {
        "id": "WLF6ZxrzvNPD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-tGxNusg0P7W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}